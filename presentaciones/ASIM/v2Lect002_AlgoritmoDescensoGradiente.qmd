---
title: "Descenso de Gradiente en Ciencias Biomédicas"
subtitle: "Regresión lineal múltiple y regresión logística"
author: "PhD. Pablo Eduardo Caicedo Rodríguez"
date: last-modified
lang: es
format:
  revealjs:
    code-tools: true
    code-overflow: wrap
    code-line-numbers: true
    code-copy: true
    fig-align: center
    self-contained: true
    theme:
      - simple
      - ../../recursos/estilos/metropolis.scss
    slide-number: true
    preview-links: auto
    logo: ../../recursos/imagenes/generales/Escuela_Rosario_logo.png
    css: ../../recursos/estilos/styles_pres.scss
    footer: <https://pablocaicedor.github.io/>
    transition: fade
    progress: true
    scrollable: true
    hash: true
  beamer:
    slide-level: 2
    incremental: false
    aspectratio: 32
    navigation: horizontal
    theme: CambridgeUS
    header-includes:
      \makeatletter
      \expandafter\let\csname figure*\endcsname\figure
      \expandafter\let\csname endfigure*\endcsname\endfigure
      \expandafter\let\csname table*\endcsname\table
      \expandafter\let\csname endtable*\endcsname\endtable
execute:
  echo: false
  warning: false
  freeze: auto
---

```{r}
#| echo: false
#| eval: true
#| output: false
#| label: Loading R-Libraries
# install.packages(c("DiagrammeR", "reticulate", "kableExtra", "tidyverse", "knitr", "cowplot", "ggfx"))
library("DiagrammeR")
library("reticulate")
library("kableExtra")
library("tidyverse")
library("knitr")
library("cowplot")
library("ggfx")
knitr::opts_chunk$set(echo = FALSE)

def.chunk.hook <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
    x <- def.chunk.hook(x, options)
    ifelse(options$size != "normalsize", paste0("\n \\", options$size, "\n\n", x, "\n\n \\normalsize"), x)
})
```

```{python}
# | echo: true
# | eval: true
# | output: false
# | label: Loading Python-Libraries

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

path_ecg = "../../data"

data_path = "../../data/"

# https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#write

```

<!-- TODO: Cambiar chunks por tabset quarto. Un tab mostrando resultado y otro tab mostrando el código -->

## Objetivos de la sesión

- Entender el **principio del descenso de gradiente (GD)** como método de optimización.
- Aplicar GD a **regresión lineal múltiple** (objetivo continuo) y **regresión logística** (clasificación 0/1).
- Analizar decisiones de ingeniería: **tasa de aprendizaje**, **escalado/normalización**, **batch vs. mini-batch vs. SGD**.
- Interpretar resultados en **contextos biomédicos** (diagnóstico, pronóstico y evaluación de riesgo).

::: notes
Estructura sugerida: 4 bloques de ~45+45+45+30 min. Actividades cortas para mantener la atención.
:::

---

## Agenda

1. Fundamentos de GD y funciones de costo
2. Caso 1: **Regresión lineal múltiple** con GD
3. Caso 2: **Regresión logística** con GD
4. Buenas prácticas, diagnósticos y discusión aplicada

---

# 1. Fundamentos de descenso de gradiente

## Motivación biomédica

- Ajustar parámetros de un **modelo fisiológico** o un **predictor clínico** con datos reales.
- Ejemplos típicos:
  - Estimar **gasto energético** a partir de IMC, edad y FC.
  - Estimar **edad vascular** con variables de laboratorio.
  - **Clasificar** presencia/ausencia de una condición (0/1) con biomarcadores.

::: notes
Enfatizar que el GD es base de la mayoría de métodos de AA actuales, incluidas redes neuronales.
:::

---

## Regresión Lineal

```{python}
# | echo: false
# | eval: true
# | output: true

data = pd.read_csv(data_path + "insurance_2.csv")
sns.regplot(
    data=data,
    x="salary",
    y="charges",
    scatter_kws={"color": "blue"},
    line_kws={"color": "red"},
)
```

## Regresión Lineal

```{python}
# | echo: false
# | eval: true
# | output: true
# | label: Error de la regresión lineal

# --- Scatter + recta de regresión (Seaborn) ---
ax = sns.regplot(
    data=data,
    x="salary",
    y="charges",
    scatter_kws={"color": "blue", "alpha": 0.1},
    line_kws={"color": "red", "lw": 2},
)

# --- Ajuste lineal para obtener m y b (residuos verticales) ---
x = data["salary"].to_numpy()
y = data["charges"].to_numpy()
mask = ~np.isnan(x) & ~np.isnan(y)
m, b = np.polyfit(x[mask], y[mask], 1)

# ====== A) Residuo para un PUNTO específico ======
idx = 100  # <-- cambia al índice deseado
x0 = data.loc[idx, "salary"]
y0 = data.loc[idx, "charges"]
yhat0 = m * x0 + b  # estimación sobre la recta

# Línea del error (vertical) + destacar el punto
ax.plot(
    [x0, x0], [y0, yhat0], color="black", linestyle="--", lw=2, label="Error (residuo)"
)
ax.scatter([x0], [y0], color="orange", s=70, zorder=5, label="Punto observado")
ax.scatter([x0], [yhat0], color="red", s=50, zorder=5, label="Estimación $\hat{y}$")

# (Opcional) Anotar el valor del residuo
resid = y0 - yhat0
ax.annotate(
    f"Residuo = {resid:.2f}",
    xy=(x0, (y0 + yhat0) / 2),
    xytext=(10, 0),
    textcoords="offset points",
    fontsize=18,
    color="black",
    fontweight="bold",
    va="center",
)

# ====== B) (Opcional) Dibujar residuos para TODOS los puntos ======
# Descomenta para ver las líneas de error de todas las observaciones
# yhat_all = m * x + b
# for xi, yi, yhi in zip(x[mask], y[mask], yhat_all[mask]):
#     ax.plot([xi, xi], [yi, yhi], color="gray", alpha=0.15)

ax.set_xlabel("salary")
ax.set_ylabel("charges")
ax.legend()
plt.tight_layout()
plt.show()

```


## Función de costo (error)

:::columns
:::{.column width="45%"}
**Regresión (continuo):**
$$
J(\mathbf{w}) \;=\; \frac{1}{2m}\sum_{i=1}^{m}\left(y_j - \hat{y}_j\right)^2
$$
donde $\hat{y}_j = \mathbf{w}^\top \mathbf{x}_j$.
:::
:::{.column .smaller width="45%"}
**Clasificación binaria:**
**Entropía cruzada** (log-loss):
$$
J(\mathbf{w}) \;=\; -\frac{1}{m}\sum_{i=1}^{m}\left[\,y_i\log\hat{p}_i + (1-y_i)\log(1-\hat{p}_i)\,\right]
$$
donde $\hat{p}_i=\sigma(\mathbf{w}^\top \mathbf{x}_i)=\frac{1}{1+e^{-\mathbf{w}^\top\mathbf{x}_i}}$.
:::
:::

::: notes
Conectar con la interpretación probabilística en logística y con el MSE en lineal.
:::

---

## Función de costo (error)
::: {.callout-important title="... Para el caso de ejemplo (un modelo tipo lineal)"}


$$
\mathbf{w_{j}}^\top \;=\; \left[w_{j,1}, w_{j,0}\right]
$$

$$
salary_i = x_i
$$

$$
\hat{y}_j  \;=\; w_{j,1}*x_j + w_{j,0}
$$

$$
J(\mathbf{w}) \;=\; \frac{1}{2m}\sum_{i=1}^{m}\left(y_j - w_{j,1}*x_i - w_{j,0}\right)^2
$$

:::

---

## Idea central del GD

- Partimos de $\mathbf{w}_{(0)}$ (p. ej., aleatorio).
- Iteramos:
$$
\mathbf{w}_{(t+1)} \;=\; \mathbf{w}_{(t)} \;-\; \alpha\, \nabla_{\mathbf{w}} J(\mathbf{w}_{(t)})
$$
- $\alpha$ = **tasa de aprendizaje**: grande → rápido pero inestable; pequeña → estable pero lento.
- Convergencia: buscamos $\nabla J \approx \mathbf{0}$.

**Visualización conceptual:** “valle” del error y trayectoria en zig-zag hacia el mínimo.

---

## Algoritmo de Descenso de Gradiente

:::{.small_font}

::: {.callout-note title="Para la estimación de $w_{j,0}$" collapsible="false"}

$$
w_{j+1,0} = w_{j,0} - \alpha \frac{\partial J}{\partial w_{j,0}}
$$

$$
\frac{\partial J}{\partial w_{j,0}} =
\frac{\partial}{\partial w_{j,0}}
\left(
\frac{1}{2m} \sum_{i=1}^{m}
\left( y_i - w_{j,1} x_i - w_{j,0} \right)^2
\right)
$$

$$
\frac{\partial}{\partial w_{j,0}}
\left(
\frac{1}{2m}
\sum_{i=1}^{m}
\left(
y_i - w_{j,1}x_i - w_{j,0}
\right)^2
\right)
=
\frac{1}{m}
\sum_{i=1}^{m}
\left(
w_{j,1}x_i + w_{j,0} - y_i
\right)
$$
:::

::: {.callout-note title="Para la estimación de $w_{j,1}$" collapsible="false"}

$$
w_{j+1,1} = w_{j,1} - \alpha \frac{\partial J}{\partial w_{j,1}}
$$

$$
\frac{\partial J}{\partial w_{j,1}} =
\frac{\partial}{\partial w_{j,1}}
\left(
\frac{1}{2m} \sum_{i=1}^{m}
\left( y_i - w_{j,1} x_i - w_{j,0} \right)^2
\right)
$$

$$
\frac{\partial J}{\partial w_{j,1}}
=
\frac{1}{m}
\sum_{i=1}^{m}
\left(
w_{j,1}x_i + w_{j,0} - y_i
\right)x_i
$$

:::

:::


---

## Variantes: batch, mini-batch, SGD

- **Batch GD:** usa todo el conjunto en cada actualización (costo alto por iteración).
- **Mini-batch GD:** usa lotes pequeños (compromiso eficiencia/ruido).
- **SGD (estocástico):** actualiza con una sola muestra por paso (barato, ruidoso, puede escapar de óptimos pobres).

**Práctica recomendada:** mini-batch (p. ej., 32–256).

---

## Preprocesamiento y escalado

- **Estandarizar** o **normalizar** las $x_j$ acelera y estabiliza GD.
- Centrar: $x_j \leftarrow (x_j - \mu_j)/\sigma_j$.
- Manejo de outliers y transformaciones (log, Box–Cox) cuando aplique.

::: notes
Conectar con mediciones biomédicas heterogéneas y escalas físicas.
:::

---

## 1.5 EDA

### Empezar a usar jupyter.

---

# 2. Caso 1 · Regresión lineal múltiple con GD

## Planteamiento

**Objetivo biomédico (ejemplo):** predecir **gasto energético** (kcal) a partir de **edad**, **IMC** y **FC**.

Modelo lineal:
$$
\hat{y} = \mathbf{w}^\top \mathbf{x} = w_0 + w_1 x_1 + \cdots + w_p x_p
$$

Costo (MSE):
$$
J(\mathbf{w}) = \frac{1}{2m}\sum_{i=1}^{m}(y_i - \hat{y}_i)^2
$$

Gradiente:
$$
\frac{\partial J}{\partial w_j} = -\frac{1}{m}\sum_{i=1}^{m}(y_i - \hat{y}_i)\,x_{ij}
$$

Actualización:
$$
w_j \leftarrow w_j - \alpha\,\frac{\partial J}{\partial w_j}
$$

---

## Pseudocódigo (mini-batch)

```pseudo
in: X (m×p), y (m), α, batch_size, epochs
preprocess: X ← standardize(X)

initialize w ← zeros(p+1)  # incluye sesgo w0 si se usa X̃ con columna 1

for epoch in 1..epochs:
    for B in iterate_minibatches(X, y, batch_size, shuffle=True):
        Xb, yb ← B
        yhat ← Xb · w
        grad ← (1/|B|) · (Xbᵀ · (yhat - yb))
        w ← w - α · grad

return w
```

::: notes
Discutir convergencia, criterio de parada (máx. iteraciones o ΔJ pequeño).
:::

---

## Diagnóstico y validación

- Curva $J$ vs. iteraciones (entrenamiento y validación).
- Errores residuales: homocedasticidad, estructura vs. predicción.
- Interpretación clínica de coeficientes $w_j$ y unidades.
- Comparar con **ecuaciones normales** (solución cerrada) y discutir condicionamiento numérico.

---

# 3. Caso 2 · Regresión logística con GD (clasificación 0/1)

## Planteamiento

**Objetivo biomédico (ejemplo):** clasificar **riesgo de enfermedad** (0/1) con panel de biomarcadores.

Modelo:
$$
\hat{p} = \sigma(\mathbf{w}^\top\mathbf{x}),\quad \sigma(z)=\frac{1}{1+e^{-z}}
$$

Costo (entropía cruzada):
$$
J(\mathbf{w}) = -\frac{1}{m}\sum_{i=1}^{m}\Big[y_i\log \hat{p}_i + (1-y_i)\log(1-\hat{p}_i)\Big]
$$

Gradiente:
$$
\frac{\partial J}{\partial w_j} = \frac{1}{m}\sum_{i=1}^{m}(\hat{p}_i - y_i)\,x_{ij}
$$

Actualización:
$$
w_j \leftarrow w_j - \alpha\,\frac{\partial J}{\partial w_j}
$$

---

## Pseudocódigo (mini-batch)

```pseudo
in: X (m×p), y∈{0,1}^m, α, batch_size, epochs
preprocess: X ← standardize(X)

initialize w ← zeros(p+1)

for epoch in 1..epochs:
    for B in iterate_minibatches(X, y, batch_size, shuffle=True):
        Xb, yb ← B
        z ← Xb · w
        p ← sigmoid(z)
        grad ← (1/|B|) · (Xbᵀ · (p - yb))
        w ← w - α · grad

return w
```

**Inferencia:** clasificar con umbral $\hat{p} \ge \tau$ (clínico/operativo).

---

## Métricas y curvas

- **AUC-ROC**, **AUPRC**, **sensibilidad**, **especificidad**, **F1**.
- Elección del **umbral $\tau$** por criterio clínico (p. ej., maximizar sensibilidad bajo límite de FPs).
- Calibración: curvas de confiabilidad.

::: notes
Relaciones costo-beneficio y prevalencia en biomédica.
:::

---

## Visualización de la frontera de decisión

- Con dos características ($x_1, x_2$), la frontera es una **línea** (hiperplano en general).
- Durante GD, la frontera rota/traslada hasta estabilizarse.
- Añadir **términos polinomiales** o **bases** para fronteras no lineales; GD sigue aplicando.

---

# 4. Buenas prácticas y discusión aplicada

## Hiperparámetros y trucos prácticos

- **Tasa de aprendizaje ($\alpha$)**: búsqueda en rejilla o **programación de tasa** (decay).
- **Inicialización**: pequeña aleatoria (cero puede estancar con ciertas variantes).
- **Barajado** por época y **mini-batches** estratificados si la clase es rara.
- **Regularización** (L2/L1) para estabilidad e interpretabilidad:
  $$
  J_{\lambda} = J + \frac{\lambda}{2}\|\mathbf{w}\|_2^2 \quad \text{(Ridge)}
  $$
- **Detección de fuga de datos** y validación por **sujeto** en estudios clínicos.

---

## Checklist de la sesión (rápido)

- [ ] Estandarizaste variables de entrada.
- [ ] Definiste costo adecuado (MSE vs. CE).
- [ ] Elegiste mini-batch y $\alpha$ razonables.
- [ ] Verificaste convergencia con curva de $J$.
- [ ] Evaluaste con métricas adecuadas al objetivo clínico.
- [ ] Documentaste supuestos y limitaciones.

---

## Objetivos de aprendizaje

- **Comprender** los fundamentos de la **Regresión Lineal**, **Regresión Logística** y **Perceptrón Multicapa (MLP)**.
- **Aplicar** estos modelos al contexto de **salud fetal** con datos de **cardiotocografía (CTG)**.
- **Evaluar** el desempeño con métricas adecuadas (MSE, AUC/Log-Loss, matriz de confusión, F1).

::: {.columns}
::: {.column width="45%"}

1. Regresión Lineal
2. Regresión Logística
3. Perceptrón Multicapa
4. Cierre y discusión

:::
::: {.column width="45%"}
**Dataset**: `fetal_health.csv` (UCI CTG).
**Contexto clínico**: interpretación de CTG (*normal*, *sospechoso*, *patológico*).
:::
:::

---

## Contexto clínico (CTG)

::: {.callout-important title="Defición"}
 Prueba médica que monitoriza simultáneamente la frecuencia cardíaca del feto y la actividad contráctil del útero. Se realiza generalmente durante el tercer trimestre del embarazo y el parto, colocando dos transductores externos (uno para la frecuencia cardíaca fetal y otro para las contracciones) sobre el abdomen de la madre
:::

![Generada con Gemini](../../recursos/imagenes/Presentaciones/ASIM/cardiotocografia.png)

## Contexto clínico (CTG)

|Característica (Variable en CSV)                      |Cálculo o Descripción                                                                                                                                                                                                                                                                                       |
|------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|Parámetros Basales                                    |                                                                                                                                                                                                                                                                                                            |
|baseline value                                        |Es la frecuencia cardíaca fetal (FCF) media aproximada en un segmento de 10 minutos, excluyendo aceleraciones, deceleraciones y períodos de variabilidad marcada (>25 lpm). Se redondea a incrementos de 5 latidos por minuto (lpm).[4, 5, 6, 7, 8] El rango normal se considera entre 110 y 160 lpm.[9, 10]|
|fetal_movement                                        |Número de movimientos fetales detectados por segundo.[1, 11, 12]                                                                                                                                                                                                                                            |
|uterine_contractions                                  |Número de contracciones uterinas por segundo. Se considera normal tener 5 o menos contracciones en 10 minutos.[1, 4, 11, 12]                                                                                                                                                                                |
|Eventos Transitorios (Aceleraciones y Deceleraciones) |                                                                                                                                                                                                                                                                                                            |
|accelerations                                         |Número de aceleraciones por segundo. Una aceleración es un aumento abrupto de la FCF por encima de la línea de base de al menos 15 lpm, que dura 15 segundos o más, pero menos de 2 minutos.[5, 9, 10]                                                                                                      |
|light_decelerations                                   |Número de deceleraciones leves por segundo. Una deceleración es una caída de la FCF de más de 15 lpm que dura más de 15 segundos.[5] La categoría "leve" se refiere a su duración, típicamente menor a 120 segundos.[3]                                                                                     |
|severe_decelerations                                  |Número de deceleraciones severas por segundo. Se refiere a deceleraciones de larga duración, a menudo definidas como aquellas que superan los 300 segundos.[3]                                                                                                                                              |
|prolongued_decelerations                              |Número de deceleraciones prolongadas por segundo. Son caídas de la FCF que duran más de 2 o 3 minutos pero menos de 10 minutos.[3, 6, 13]                                                                                                                                                                   |
|Variabilidad de la FCF                                |                                                                                                                                                                                                                                                                                                            |
|abnormal_short_term_variability                       |Porcentaje de tiempo en que la variabilidad a corto plazo (latido a latido) es anormal. La variabilidad se considera anormal si es mínima (≤5 lpm) o marcada (>25 lpm).[6, 8]                                                                                                                               |
|mean_value_of_short_term_variability                  |Valor medio de la variabilidad a corto plazo (STV), que describe las fluctuaciones de la FCF latido a latido.[3, 6]                                                                                                                                                                                         |
|percentage_of_time_with_abnormal_long_term_variability|Porcentaje de tiempo en que la variabilidad a largo plazo es anormal. Se calcula sobre las fluctuaciones de la FCF en un período de un minuto.[5]                                                                                                                                                           |
|mean_value_of_long_term_variability                   |Valor medio de la variabilidad a largo plazo (LTV), que mide la amplitud (diferencia entre el pico y el valle) de las fluctuaciones de la FCF en un minuto.[3, 5]                                                                                                                                           |
|Características del Histograma de FCF                 |Estas son propiedades estadísticas calculadas a partir de la distribución de todos los valores de FCF registrados durante el período de monitorización.[1, 11, 12]                                                                                                                                          |
|histogram_width                                       |El ancho del histograma, calculado como la diferencia entre el valor máximo (histogram_max) y el mínimo (histogram_min) de la FCF.                                                                                                                                                                          |
|histogram_min                                         |El valor mínimo de la FCF registrado en el histograma.                                                                                                                                                                                                                                                      |
|histogram_max                                         |El valor máximo de la FCF registrado en el histograma.                                                                                                                                                                                                                                                      |
|histogram_number_of_peaks                             |El número de picos en la distribución del histograma.                                                                                                                                                                                                                                                       |
|histogram_number_of_zeroes                            |El número de "ceros" o bins con frecuencia cero en el histograma.                                                                                                                                                                                                                                           |
|histogram_mode                                        |El valor de FCF que aparece con mayor frecuencia (la moda estadística).                                                                                                                                                                                                                                     |
|histogram_mean                                        |El valor medio de la FCF en el histograma (la media estadística).                                                                                                                                                                                                                                           |
|histogram_median                                      |El valor central de la FCF en el histograma (la mediana estadística).                                                                                                                                                                                                                                       |
|histogram_variance                                    |La varianza de los valores de FCF, que mide su dispersión alrededor de la media.                                                                                                                                                                                                                            |
|histogram_tendency                                    |Indica la simetría o sesgo del histograma. Puede interpretarse como: 1 para tendencia a la derecha (positiva), -1 para tendencia a la izquierda (negativa) y 0 para una distribución simétrica.                                                                                                             |


## Contexto clínico (CTG)

- CTG registra **FCF** y **contracciones uterinas**.
- Clasificación clínica (FIGO): **normal / sospechoso / patológico**.
- Variabilidad, aceleraciones y desaceleraciones son claves.



---

## Regresión Lineal

### Idea clave
Aproxima una relación **lineal** $\hat{y} = \beta_0 + \sum_j \beta_j x_j$minimizando **MSE**.

### Ejemplo didáctico (CTG)
Usamos una **variable continua** de CTG como respuesta (p. ej., *histogram_width*) para ilustrar ajuste y residuales.

```{python}
# | echo: true
# | eval: true
# | output: true
import pandas as pd, numpy as np, matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
df = pd.read_csv("./data/Fetal Health Classification/fetal_health.csv")

X = df[["accelerations"]].values
y = df["histogram_width"].values

model = LinearRegression().fit(X, y)
x0 = np.linspace(X.min(), X.max(), 100).reshape(-1,1)
yhat = model.predict(x0)

plt.figure()
plt.scatter(X, y, s=10)
plt.plot(x0, yhat)
plt.title("Regresión lineal: histogram_width vs accelerations")
plt.xlabel("accelerations")
plt.ylabel("histogram_width")
plt.tight_layout()
plt.show()
```

**Discusión:** supuestos (linealidad, homocedasticidad, independencia), diagnóstico con residuales.

---

## Regresión Logística

### Idea clave
Modela $P(Y=1 \mid \mathbf{x}) = \sigma(\beta_0 + \mathbf{x}^\top \beta)$ con **sigmoide** $\sigma(z)=1/(1+e^{-z})$.

## 1. Definición de Regresión Logística

La **Regresión Logística** es un algoritmo de aprendizaje automático supervisado utilizado fundamentalmente para problemas de **clasificación binaria**.

A pesar de su nombre, su objetivo no es predecir un valor continuo, sino modelar la **probabilidad** ($P$) de que una observación pertenezca a una clase específica (usualmente denotada como $Y=1$).

El modelo toma variables de entrada (features) $x_1, \dots, x_n$ y estima $P(Y=1 | \mathbf{x})$.

---

## 2. El Mecanismo Central del Modelo

El modelo logístico opera en dos pasos cruciales:

### 2.1. El Componente Lineal (Logit)

Primero, el modelo calcula una suma ponderada de las entradas, exactamente igual que en una regresión lineal. A este resultado ($z$) se le conoce como **logit** o, más formalmente, **log-odds**.

$$
z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n
$$

* $\beta_0$ es el intercepto (sesgo).
* $\beta_{1 \dots n}$ son los coeficientes (pesos) que el modelo aprende.
* El rango de salida de $z$ es el de todos los números reales: $(-\infty, +\infty)$.

### 2.2. La Función Sigmoide (Logística)

Dado que una probabilidad debe estar en el rango $[0, 1]$, $z$ no puede ser el resultado final. La regresión logística aplica la **función sigmoide** ($\sigma$) a $z$ para "aplastar" (squash) la salida lineal al rango de probabilidad.

$$
P = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

* Si $z \to +\infty$, $e^{-z} \to 0$, y $P \to 1$.
* Si $z \to -\infty$, $e^{-z} \to +\infty$, y $P \to 0$.
* Si $z = 0$, $e^{-0} = 1$, y $P = 0.5$.

---

## 3. La Relación Clave: Probabilidad y Log-Odds

El concepto central que conecta el modelo lineal con la probabilidad es el **log-odds**. Esta transformación es necesaria para mapear un espacio acotado $[0, 1]$ a un espacio no acotado $[-\infty, +\infty]$.

### 3.1. De Probabilidad a Log-Odds

La transformación se realiza en dos pasos:

1.  **Probabilidad ($P$)**: La probabilidad del evento.
    * Rango: $[0, 1]$

2.  **Odds (Momios)**: La razón entre la probabilidad de que ocurra ($P$) y la de que no ocurra ($1-P$).
    $$
    Odds = \frac{P}{1-P}
    $$
    * Rango: $[0, +\infty]$

3.  **Log-Odds (Logit)**: El logaritmo natural de los *odds*.
    $$
    Logit(P) = \ln(Odds) = \ln\left(\frac{P}{1-P}\right)
    $$
    * Rango: $[-\infty, +\infty]$

El modelo de regresión logística es, por tanto, un modelo lineal que predice el *log-odds*:

$$
z = \ln\left(\frac{P}{1-P}\right) = \beta_0 + \beta_1 x_1 + \dots + \beta_n x_n
$$

### 3.2. De Log-Odds a Probabilidad (La Inversa)

Para obtener la probabilidad $P$ a partir del log-odds $z$, simplemente revertimos la transformación `Logit`. Este proceso de despejar $P$ de la ecuación del logit **da origen a la función sigmoide**:

1.  Ecuación base:
    $$
    z = \ln\left(\frac{P}{1-P}\right)
    $$

2.  Aplicar exponencial (inversa del logaritmo):
    $$
    e^z = \frac{P}{1-P}
    $$

3.  Despejar $P$:
    $$
    e^z (1-P) = P
    $$
    $$
    e^z - e^z P = P
    $$
    $$
    e^z = P + e^z P
    $$
    $$
    e^z = P (1 + e^z)
    $$

4.  Probabilidad $P$ en función de $z$:
    $$
    P = \frac{e^z}{1 + e^z}
    $$

5.  *Forma sigmoide alternativa (dividiendo numerador y denominador por $e^z$)*:
    $$
    P = \frac{e^z/e^z}{(1 + e^z)/e^z} = \frac{1}{e^{-z} + 1} = \frac{1}{1 + e^{-z}}
    $$

---

## 4. Interpretación de Coeficientes

Debido a esta relación, los coeficientes ($\beta_i$) del modelo tienen una interpretación específica:

* **Coeficiente $\beta_i$**: Un incremento de una unidad en la variable $x_i$ (manteniendo las demás constantes) genera un cambio de $\beta_i$ en el **log-odds** de la predicción.
* **Odds Ratio (OR)**: Para una interpretación más intuitiva, se utiliza $e^{\beta_i}$. Un incremento de una unidad en $x_i$ **multiplica** los *odds* por un factor de $e^{\beta_i}$.

```{python}
# | echo: true
# | eval: true
# | output: true
import numpy as np
import matplotlib.pyplot as plt

z = np.linspace(-8, 8, 400)
sig = 1/(1+np.exp(-z))

plt.figure()
plt.plot(z, sig)
plt.title("Función sigmoide")
plt.xlabel("z")
plt.ylabel("σ(z)")
plt.tight_layout(); plt.show()
```

### Clasificación binaria (Normal vs No‑Normal)
```{python}
# | echo: true
# | eval: true
# | output: true
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv("./data/Fetal Health Classification/fetal_health.csv")
df["is_normal"] = (df["fetal_health"]==1).astype(int)

X = df.drop(columns=["fetal_health","is_normal"]).values
y = df["is_normal"].values

X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
sc = StandardScaler().fit(X_tr); X_tr_s, X_te_s = sc.transform(X_tr), sc.transform(X_te)

logit = LogisticRegression(max_iter=200).fit(X_tr_s, y_tr)
proba = logit.predict_proba(X_te_s)[:,1]
pred = (proba>=0.5).astype(int)

print(classification_report(y_te, pred, digits=3))
cm = confusion_matrix(y_te, pred)

plt.figure()
plt.imshow(cm, cmap="gray")
plt.title("Matriz de confusión (Logística binaria)")
plt.xlabel("Predicho"); plt.ylabel("Real")
for i in range(2):
    for j in range(2):
        plt.text(j, i, cm[i, j], ha="center", va="center")
plt.tight_layout(); plt.show()
```

---
