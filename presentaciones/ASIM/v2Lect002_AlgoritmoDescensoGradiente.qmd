---
title: "Descenso de Gradiente en Ciencias Biomédicas"
subtitle: "Regresión lineal múltiple y regresión logística"
author: "PhD. Pablo Eduardo Caicedo Rodríguez"
date: last-modified
lang: es
format:
  revealjs:
    code-tools: true
    code-overflow: wrap
    code-line-numbers: true
    code-copy: true
    fig-align: center
    self-contained: true
    theme:
      - simple
      - ../../recursos/estilos/metropolis.scss
    slide-number: true
    preview-links: auto
    logo: ../../recursos/imagenes/generales/Escuela_Rosario_logo.png
    css: ../../recursos/estilos/styles_pres.scss
    footer: <https://pablocaicedor.github.io/>
    transition: fade
    progress: true
    scrollable: true
    hash: true
  beamer:
    slide-level: 2
    incremental: false
    aspectratio: 32
    navigation: horizontal
    theme: CambridgeUS
    header-includes:
      \makeatletter
      \expandafter\let\csname figure*\endcsname\figure
      \expandafter\let\csname endfigure*\endcsname\endfigure
      \expandafter\let\csname table*\endcsname\table
      \expandafter\let\csname endtable*\endcsname\endtable
execute:
  echo: false
  warning: false
  freeze: auto
---

```{r}
#| echo: false
#| eval: true
#| output: false
#| label: Loading R-Libraries
# install.packages(c("DiagrammeR", "reticulate", "kableExtra", "tidyverse", "knitr", "cowplot", "ggfx"))
library("DiagrammeR")
library("reticulate")
library("kableExtra")
library("tidyverse")
library("knitr")
library("cowplot")
library("ggfx")
knitr::opts_chunk$set(echo = FALSE)

def.chunk.hook <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
    x <- def.chunk.hook(x, options)
    ifelse(options$size != "normalsize", paste0("\n \\", options$size, "\n\n", x, "\n\n \\normalsize"), x)
})
```

```{python}
# | echo: true
# | eval: true
# | output: false
# | label: Loading Python-Libraries

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

path_ecg = "../../data"

data_path = "../../data/"

# https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#write

```

<!-- TODO: Cambiar chunks por tabset quarto. Un tab mostrando resultado y otro tab mostrando el código -->

## Objetivos de la sesión

- Entender el **principio del descenso de gradiente (GD)** como método de optimización.
- Aplicar GD a **regresión lineal múltiple** (objetivo continuo) y **regresión logística** (clasificación 0/1).
- Analizar decisiones de ingeniería: **tasa de aprendizaje**, **escalado/normalización**, **batch vs. mini-batch vs. SGD**.
- Interpretar resultados en **contextos biomédicos** (diagnóstico, pronóstico y evaluación de riesgo).

::: notes
Estructura sugerida: 4 bloques de ~45+45+45+30 min. Actividades cortas para mantener la atención.
:::

---

## Agenda

1. Fundamentos de GD y funciones de costo
2. Caso 1: **Regresión lineal múltiple** con GD
3. Caso 2: **Regresión logística** con GD
4. Buenas prácticas, diagnósticos y discusión aplicada

---

# 1. Fundamentos de descenso de gradiente

## Motivación biomédica

- Ajustar parámetros de un **modelo fisiológico** o un **predictor clínico** con datos reales.
- Ejemplos típicos:
  - Estimar **gasto energético** a partir de IMC, edad y FC.
  - Estimar **edad vascular** con variables de laboratorio.
  - **Clasificar** presencia/ausencia de una condición (0/1) con biomarcadores.

::: notes
Enfatizar que el GD es base de la mayoría de métodos de AA actuales, incluidas redes neuronales.
:::

---

## Regresión Lineal

```{python}
# | echo: false
# | eval: true
# | output: true

data = pd.read_csv(data_path + "insurance_2.csv")
sns.regplot(
    data=data,
    x="salary",
    y="charges",
    scatter_kws={"color": "blue"},
    line_kws={"color": "red"},
)
```

## Regresión Lineal

```{python}
# | echo: false
# | eval: true
# | output: true
# | label: Error de la regresión lineal

# --- Scatter + recta de regresión (Seaborn) ---
ax = sns.regplot(
    data=data,
    x="salary",
    y="charges",
    scatter_kws={"color": "blue", "alpha": 0.1},
    line_kws={"color": "red", "lw": 2},
)

# --- Ajuste lineal para obtener m y b (residuos verticales) ---
x = data["salary"].to_numpy()
y = data["charges"].to_numpy()
mask = ~np.isnan(x) & ~np.isnan(y)
m, b = np.polyfit(x[mask], y[mask], 1)

# ====== A) Residuo para un PUNTO específico ======
idx = 100  # <-- cambia al índice deseado
x0 = data.loc[idx, "salary"]
y0 = data.loc[idx, "charges"]
yhat0 = m * x0 + b  # estimación sobre la recta

# Línea del error (vertical) + destacar el punto
ax.plot(
    [x0, x0], [y0, yhat0], color="black", linestyle="--", lw=2, label="Error (residuo)"
)
ax.scatter([x0], [y0], color="orange", s=70, zorder=5, label="Punto observado")
ax.scatter([x0], [yhat0], color="red", s=50, zorder=5, label="Estimación $\hat{y}$")

# (Opcional) Anotar el valor del residuo
resid = y0 - yhat0
ax.annotate(
    f"Residuo = {resid:.2f}",
    xy=(x0, (y0 + yhat0) / 2),
    xytext=(10, 0),
    textcoords="offset points",
    fontsize=18,
    color="black",
    fontweight="bold",
    va="center",
)

# ====== B) (Opcional) Dibujar residuos para TODOS los puntos ======
# Descomenta para ver las líneas de error de todas las observaciones
# yhat_all = m * x + b
# for xi, yi, yhi in zip(x[mask], y[mask], yhat_all[mask]):
#     ax.plot([xi, xi], [yi, yhi], color="gray", alpha=0.15)

ax.set_xlabel("salary")
ax.set_ylabel("charges")
ax.legend()
plt.tight_layout()
plt.show()

```


## Función de costo (error)

:::columns
:::{.column width="45%"}
**Regresión (continuo):**
$$
J(\mathbf{w}) \;=\; \frac{1}{2m}\sum_{i=1}^{m}\left(y_j - \hat{y}_j\right)^2
$$
donde $\hat{y}_j = \mathbf{w}^\top \mathbf{x}_j$.
:::
:::{.column .smaller width="45%"}
**Clasificación binaria:**
**Entropía cruzada** (log-loss):
$$
J(\mathbf{w}) \;=\; -\frac{1}{m}\sum_{i=1}^{m}\left[\,y_i\log\hat{p}_i + (1-y_i)\log(1-\hat{p}_i)\,\right]
$$
donde $\hat{p}_i=\sigma(\mathbf{w}^\top \mathbf{x}_i)=\frac{1}{1+e^{-\mathbf{w}^\top\mathbf{x}_i}}$.
:::
:::

::: notes
Conectar con la interpretación probabilística en logística y con el MSE en lineal.
:::

---

## Función de costo (error)
::: {.callout-important title="... Para el caso de ejemplo (un modelo tipo lineal)"}


$$
\mathbf{w_{j}}^\top \;=\; \left[w_{j,1}, w_{j,0}\right]
$$

$$
salary_i = x_i
$$

$$
\hat{y}_j  \;=\; w_{j,1}*x_j + w_{j,0}
$$

$$
J(\mathbf{w}) \;=\; \frac{1}{2m}\sum_{i=1}^{m}\left(y_j - w_{j,1}*x_i - w_{j,0}\right)^2
$$

:::

---

## Idea central del GD

- Partimos de $\mathbf{w}_{(0)}$ (p. ej., aleatorio).
- Iteramos:
$$
\mathbf{w}_{(t+1)} \;=\; \mathbf{w}_{(t)} \;-\; \alpha\, \nabla_{\mathbf{w}} J(\mathbf{w}_{(t)})
$$
- $\alpha$ = **tasa de aprendizaje**: grande → rápido pero inestable; pequeña → estable pero lento.
- Convergencia: buscamos $\nabla J \approx \mathbf{0}$.

**Visualización conceptual:** “valle” del error y trayectoria en zig-zag hacia el mínimo.

---

## Algoritmo de Descenso de Gradiente

:::{.small_font}

::: {.callout-note title="Para la estimación de $w_{j,0}$" collapsible="false"}

$$
w_{j+1,0} = w_{j,0} - \alpha \frac{\partial J}{\partial w_{j,0}}
$$

$$
\frac{\partial J}{\partial w_{j,0}} =
\frac{\partial}{\partial w_{j,0}}
\left(
\frac{1}{2m} \sum_{i=1}^{m}
\left( y_i - w_{j,1} x_i - w_{j,0} \right)^2
\right)
$$

$$
\frac{\partial}{\partial w_{j,0}}
\left(
\frac{1}{2m}
\sum_{i=1}^{m}
\left(
y_i - w_{j,1}x_i - w_{j,0}
\right)^2
\right)
=
\frac{1}{m}
\sum_{i=1}^{m}
\left(
w_{j,1}x_i + w_{j,0} - y_i
\right)
$$
:::

::: {.callout-note title="Para la estimación de $w_{j,1}$" collapsible="false"}

$$
w_{j+1,1} = w_{j,1} - \alpha \frac{\partial J}{\partial w_{j,1}}
$$

$$
\frac{\partial J}{\partial w_{j,1}} =
\frac{\partial}{\partial w_{j,1}}
\left(
\frac{1}{2m} \sum_{i=1}^{m}
\left( y_i - w_{j,1} x_i - w_{j,0} \right)^2
\right)
$$

$$
\frac{\partial J}{\partial w_{j,1}}
=
\frac{1}{m}
\sum_{i=1}^{m}
\left(
w_{j,1}x_i + w_{j,0} - y_i
\right)x_i
$$

:::

:::


---

## Variantes: batch, mini-batch, SGD

- **Batch GD:** usa todo el conjunto en cada actualización (costo alto por iteración).
- **Mini-batch GD:** usa lotes pequeños (compromiso eficiencia/ruido).
- **SGD (estocástico):** actualiza con una sola muestra por paso (barato, ruidoso, puede escapar de óptimos pobres).

**Práctica recomendada:** mini-batch (p. ej., 32–256).

---

## Preprocesamiento y escalado

- **Estandarizar** o **normalizar** las $x_j$ acelera y estabiliza GD.
- Centrar: $x_j \leftarrow (x_j - \mu_j)/\sigma_j$.
- Manejo de outliers y transformaciones (log, Box–Cox) cuando aplique.

::: notes
Conectar con mediciones biomédicas heterogéneas y escalas físicas.
:::

---

## 1.5 EDA

### Empezar a usar jupyter.

---

# 2. Caso 1 · Regresión lineal múltiple con GD

## Planteamiento

**Objetivo biomédico (ejemplo):** predecir **gasto energético** (kcal) a partir de **edad**, **IMC** y **FC**.

Modelo lineal:
$$
\hat{y} = \mathbf{w}^\top \mathbf{x} = w_0 + w_1 x_1 + \cdots + w_p x_p
$$

Costo (MSE):
$$
J(\mathbf{w}) = \frac{1}{2m}\sum_{i=1}^{m}(y_i - \hat{y}_i)^2
$$

Gradiente:
$$
\frac{\partial J}{\partial w_j} = -\frac{1}{m}\sum_{i=1}^{m}(y_i - \hat{y}_i)\,x_{ij}
$$

Actualización:
$$
w_j \leftarrow w_j - \alpha\,\frac{\partial J}{\partial w_j}
$$

---

## Pseudocódigo (mini-batch)

```pseudo
in: X (m×p), y (m), α, batch_size, epochs
preprocess: X ← standardize(X)

initialize w ← zeros(p+1)  # incluye sesgo w0 si se usa X̃ con columna 1

for epoch in 1..epochs:
    for B in iterate_minibatches(X, y, batch_size, shuffle=True):
        Xb, yb ← B
        yhat ← Xb · w
        grad ← (1/|B|) · (Xbᵀ · (yhat - yb))
        w ← w - α · grad

return w
```

::: notes
Discutir convergencia, criterio de parada (máx. iteraciones o ΔJ pequeño).
:::

---

## Diagnóstico y validación

- Curva $J$ vs. iteraciones (entrenamiento y validación).
- Errores residuales: homocedasticidad, estructura vs. predicción.
- Interpretación clínica de coeficientes $w_j$ y unidades.
- Comparar con **ecuaciones normales** (solución cerrada) y discutir condicionamiento numérico.

---

# 3. Caso 2 · Regresión logística con GD (clasificación 0/1)

## Planteamiento

**Objetivo biomédico (ejemplo):** clasificar **riesgo de enfermedad** (0/1) con panel de biomarcadores.

Modelo:
$$
\hat{p} = \sigma(\mathbf{w}^\top\mathbf{x}),\quad \sigma(z)=\frac{1}{1+e^{-z}}
$$

Costo (entropía cruzada):
$$
J(\mathbf{w}) = -\frac{1}{m}\sum_{i=1}^{m}\Big[y_i\log \hat{p}_i + (1-y_i)\log(1-\hat{p}_i)\Big]
$$

Gradiente:
$$
\frac{\partial J}{\partial w_j} = \frac{1}{m}\sum_{i=1}^{m}(\hat{p}_i - y_i)\,x_{ij}
$$

Actualización:
$$
w_j \leftarrow w_j - \alpha\,\frac{\partial J}{\partial w_j}
$$

---

## Pseudocódigo (mini-batch)

```pseudo
in: X (m×p), y∈{0,1}^m, α, batch_size, epochs
preprocess: X ← standardize(X)

initialize w ← zeros(p+1)

for epoch in 1..epochs:
    for B in iterate_minibatches(X, y, batch_size, shuffle=True):
        Xb, yb ← B
        z ← Xb · w
        p ← sigmoid(z)
        grad ← (1/|B|) · (Xbᵀ · (p - yb))
        w ← w - α · grad

return w
```

**Inferencia:** clasificar con umbral $\hat{p} \ge \tau$ (clínico/operativo).

---

## Métricas y curvas

- **AUC-ROC**, **AUPRC**, **sensibilidad**, **especificidad**, **F1**.
- Elección del **umbral $\tau$** por criterio clínico (p. ej., maximizar sensibilidad bajo límite de FPs).
- Calibración: curvas de confiabilidad.

::: notes
Relaciones costo-beneficio y prevalencia en biomédica.
:::

---

## Visualización de la frontera de decisión

- Con dos características ($x_1, x_2$), la frontera es una **línea** (hiperplano en general).
- Durante GD, la frontera rota/traslada hasta estabilizarse.
- Añadir **términos polinomiales** o **bases** para fronteras no lineales; GD sigue aplicando.

---

# 4. Buenas prácticas y discusión aplicada

## Hiperparámetros y trucos prácticos

- **Tasa de aprendizaje ($\alpha$)**: búsqueda en rejilla o **programación de tasa** (decay).
- **Inicialización**: pequeña aleatoria (cero puede estancar con ciertas variantes).
- **Barajado** por época y **mini-batches** estratificados si la clase es rara.
- **Regularización** (L2/L1) para estabilidad e interpretabilidad:
  $$
  J_{\lambda} = J + \frac{\lambda}{2}\|\mathbf{w}\|_2^2 \quad \text{(Ridge)}
  $$
- **Detección de fuga de datos** y validación por **sujeto** en estudios clínicos.

---

## Checklist de la sesión (rápido)

- [ ] Estandarizaste variables de entrada.
- [ ] Definiste costo adecuado (MSE vs. CE).
- [ ] Elegiste mini-batch y $\alpha$ razonables.
- [ ] Verificaste convergencia con curva de $J$.
- [ ] Evaluaste con métricas adecuadas al objetivo clínico.
- [ ] Documentaste supuestos y limitaciones.

---
