---
title: "Evaluación de la Calidad de los Modelos"
subtitle: "Evaluación de modelos clasificatorios y de regresión"
description: "ASIM"
lang: es
author: "Ph.D. Pablo Eduardo Caicedo Rodríguez"
date: "`r Sys.Date()`"
format:
  html:
    code-tools: true
    code-overflow: scroll
    code-line-numbers: true
    code-copy: true
    fig-align: center
    self-contained: true
    theme:
      - simple
      - ../../../recursos/estilos/metropolis.scss
    slide-number: true
    preview-links: auto
    logo: ../../../recursos/imagenes/generales/Escuela_Rosario_logo.png
    css: ../../../recursos/estilos/styles_pres.scss
    footer: <https://pablocaicedor.github.io/>
    transition: fade
    progress: true
    scrollable: true

resources:
  - demo.pdf
---

```{r}
#| echo: false
#| eval: true
#| output: false
#| label: Loading R-Libraries
# install.packages(c("DiagrammeR", "reticulate", "kableExtra", "tidyverse", "knitr", "cowplot", "ggfx"))
library("DiagrammeR")
library("reticulate")
library("kableExtra")
library("tidyverse")
library("knitr")
library("cowplot")
library("ggfx")
knitr::opts_chunk$set(echo = FALSE)

def.chunk.hook <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
    x <- def.chunk.hook(x, options)
    ifelse(options$size != "normalsize", paste0("\n \\", options$size, "\n\n", x, "\n\n \\normalsize"), x)
})
```

```{python}
#| echo: false
#| eval: true
#| output: false
#| label: Loading Python-Libraries

import numpy as np
import matplotlib.pyplot as plt
path_ecg="../../data"

#https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#write

```

**Resumen Ejecutivo (Abstract)**
Este reporte establece un marco de referencia conceptual y matemático para la evaluación de algoritmos de machine learning en tareas de clasificación y regresión. Se analiza rigurosamente el propósito estadístico de las métricas de evaluación (Clasificación: Matriz de Confusión, Accuracy, Precision, Recall, F1-Score; Regresión: MAE, MSE, RMSE, R²) y las metodologías de validación (Train/Test Split, K-Fold, Stratified K-Fold, LOOCV). El análisis se centra en las propiedades matemáticas de cada métrica, sus supuestos subyacentes y sus limitaciones prácticas, como la sensibilidad a *outliers* y el comportamiento en datasets desbalanceados. Se concluye con un análisis teórico comparativo de las técnicas de validación, examinando el *tradeoff* de sesgo-varianza en la estimación del error de generalización, proporcionando una base para la selección de protocolos de evaluación robustos.

---

## I. Principios Fundamentales de la Evaluación de Modelos

### Sección 1.1 El Problema Central: Estimación del Error de Generalización

El objetivo axiomático del machine learning supervisado no es la memorización de los datos de entrenamiento (rendimiento *in-sample*), sino la capacidad de *generalización* del modelo a datos futuros, no observados (rendimiento *out-of-sample*).[1, 2] El fenómeno del *sobreajuste* (overfitting) se define como el escenario en el cual un modelo se adapta excesivamente a las idiosincrasias estocásticas (ruido) del conjunto de entrenamiento, resultando en una degradación de su rendimiento predictivo en nuevos datos.[3]

Por lo tanto, la evaluación es el proceso mediante el cual se estima el rendimiento de un modelo en datos no vistos, con el fin de seleccionar el modelo que posea la mejor capacidad de generalización.[1, 3]

### Sección 1.2 Definiciones Formales: Riesgo Esperado vs. Riesgo Empírico

Desde un punto de vista estadístico formal, la evaluación es un problema de estimación. Sea $D = \{(x_i, y_i)\}_{i=1}^n$ un conjunto de datos muestreado de una distribución de probabilidad verdadera pero desconocida, $P(x, y)$. Sea $f$ nuestro modelo (o hipótesis) y sea $L(f(x), y)$ una *función de pérdida* (Loss Function) que cuantifica el costo de predecir $f(x)$ cuando el valor real es $y$.

**Riesgo Esperado (Error de Generalización, $R(f)$):**
Este es el verdadero error del modelo sobre la distribución de datos subyacente $P$. Se define como el valor esperado de la función de pérdida:
$$R(f) = \mathbb{E}_{P(x,y)}[L(f(x), y)] = \int L(f(x), y) dP(x, y)$$
Este valor es el objetivo real de nuestra optimización, pero es *incomputable* en la práctica, ya que no conocemos la distribución $P(x, y)$.[4]

**Riesgo Empírico (Error de Entrenamiento, $R_{emp}(f)$):**
Este es el error promedio medido sobre nuestro conjunto de entrenamiento muestreado $D$.[4] Es el sustituto (proxy) que podemos calcular:
$$R_{emp}(f) = \frac{1}{n} \sum_{i=1}^n L(f(x_i), y_i)$$

### Sección 1.3 El Propósito Unificado de la Evaluación

Todo el campo de la evaluación de modelos (que abarca tanto las métricas como las técnicas de validación) puede unificarse bajo un único propósito: la búsqueda de un *estimador estadístico* fiable para el *Riesgo Esperado* ($R(f)$) incomputable.

1.  Las **Métricas** (ej. MSE, Accuracy) son la *elección de la función de pérdida $L$* que se considera relevante para el problema en el cálculo del riesgo.
2.  Las **Técnicas de Validación** (ej. K-Fold) son el *proceso de muestreo* (resampling) que se utiliza para calcular una estimación del riesgo (ej. el error de prueba o el error de validación cruzada) que sea menos sesgada que el Riesgo Empírico.

El error de entrenamiento ($R_{emp}$) es conocido por ser un estimador *optimistamente sesgado* del error de generalización ($R(f)$), especialmente en modelos con alta capacidad (complejos), ya que el modelo ha sido optimizado directamente sobre esos datos.[3] Las técnicas de validación (Sección IV) son, por lo tanto, metodologías diseñadas para obtener un estimador más preciso e insesgado del verdadero rendimiento del modelo.[2]

---

## II. Métricas de Evaluación para Problemas de Clasificación

En los problemas de clasificación, la función de pérdida $L$ no suele ser continua, sino que se basa en el conteo de predicciones correctas e incorrectas.

### Sección 2.1 La Matriz de Confusión como Base Analítica

La Matriz de Confusión no es una métrica de rendimiento *per se*, sino una *desagregación* tabular exhaustiva de los resultados de un modelo de clasificación, permitiendo un análisis detallado de los tipos de error.[5, 6] Para un problema de clasificación binaria (Clase Positiva vs. Clase Negativa), la matriz 2x2 se define mediante cuatro conteos atómicos [5, 7]:

*   **Verdadero Positivo (TP):** El modelo predijo 'Positivo' y la etiqueta real era 'Positivo'.[7, 8]
*   **Verdadero Negativo (TN):** El modelo predijo 'Negativo' y la etiqueta real era 'Negativo'.[7, 8]
*   **Falso Positivo (FP) - Error Tipo I:** El modelo predijo 'Positivo' pero la etiqueta real era 'Negativo'.[5, 7, 8]
*   **Falso Negativo (FN) - Error Tipo II:** El modelo predijo 'Negativo' pero la etiqueta real era 'Positivo'.[5, 7, 8]

**Tabla 1: Matriz de Confusión Binaria**

| | **Predicho: Positivo** | **Predicho: Negativo** | **Total Real** |
| :--- | :---: | :---: | :---: |
| **Real: Positivo** | $TP$ | $FN$ | $P = TP + FN$ |
| **Real: Negativo** | $FP$ | $TN$ | $N = FP + TN$ |
| **Total Predicho** | $TP + FP$ | $FN + TN$ | $P + N$ |

El valor fundamental de la matriz de confusión es que permite un análisis de *costos asimétricos*.[9] Todas las métricas de clasificación (Accuracy, Precision, Recall) son simplemente *funciones de agregación* de estos cuatro valores. La selección de la métrica apropiada depende enteramente del *costo relativo* de los Errores Tipo I (FP) vs. Tipo II (FN) en el dominio del problema.[9]

*   **Ejemplo 1: Filtro de Spam**.[8, 9] Un Falso Positivo (FP) ocurre cuando un email legítimo es clasificado como spam. Un Falso Negativo (FN) es un spam que llega al buzón. El costo de un FP (perder un email importante) es mucho *más alto* que el de un FN (borrar un email de spam). Por lo tanto, se prioriza minimizar los FP.
*   **Ejemplo 2: Diagnóstico Médico.** Un Falso Negativo (FN) ocurre cuando un paciente enfermo es diagnosticado como sano. Un Falso Positivo (FP) es un paciente sano diagnosticado como enfermo. El costo de un FN (falta de tratamiento) es *catastróficamente más alto* que el de un FP (realizar más pruebas). Se prioriza minimizar los FN.

### Sección 2.2 Métricas Derivadas y su Interpretación Estadística

#### 2.2.1 Accuracy (Exactitud)

*   **Definición Matemática:** La proporción de predicciones correctas (positivas y negativas) sobre el número total de predicciones.[9]
    $$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$
*   **Interpretación:** "¿Qué fracción del total de predicciones fue correcta?".[9]

#### 2.2.2 La Falacia de la Exactitud: La Paradoja de los Datasets Desbalanceados

El *Accuracy* es la métrica más intuitiva, pero es profundamente engañosa e inapropiada para problemas con *desbalance de clases*.[9, 10, 11, 12] Un desbalance de clases ocurre cuando una clase (la mayoritaria) es mucho más frecuente que la otra (la minoritaria).[13]

La inutilidad del *Accuracy* en estos escenarios [11, 14, 15] se puede demostrar formalmente:

Considérese un dataset de $N=1000$ muestras para la detección de una enfermedad rara, donde el 99% de la población está sana (Clase Negativa) y el 1% está enferma (Clase Positiva).
*   Total de Muestras: 1000
*   Reales Negativos (N): 990
*   Reales Positivos (P): 10

Ahora, considérese un modelo trivial (e inútil) que *siempre predice 'Negativo'* para cualquier entrada.
Evaluemos este modelo usando la Matriz de Confusión:
*   $TP = 0$ (nunca predice 'Positivo')
*   $FP = 0$ (nunca predice 'Positivo')
*   $TN = 990$ (predijo 'Negativo' para los 990 sanos, y acertó)
*   $FN = 10$ (predijo 'Negativo' para los 10 enfermos, y falló)

Si calculamos el *Accuracy* de este modelo:
$$Accuracy = \frac{TP + TN}{Total} = \frac{0 + 990}{1000} = 0.99$$
El modelo obtiene un 99% de *Accuracy* [11, 15], lo que sugiere un rendimiento excelente. Sin embargo, el modelo es *completamente inútil* [9], ya que su habilidad para identificar la clase de interés (la positiva) es nula. El valor del *Accuracy* está *dominado* por el término $TN$ (la habilidad de identificar correctamente la clase mayoritaria) [14], ocultando el fallo total en la clase minoritaria.

#### 2.2.3 Precision (Precisión)

*   **Definición Matemática:** También conocido como Valor Predictivo Positivo (PPV).[16] Es la fracción de predicciones *positivas* que fueron realmente correctas.[9, 10]
    $$Precision = \frac{TP}{TP + FP}$$
*   **Interpretación Conceptual:** "De todas las veces que el modelo *dijo* 'Positivo', ¿qué porcentaje *realmente* era 'Positivo'?".[10]
*   **Contexto de Uso:** Métrica crítica cuando el costo de un Falso Positivo (FP) es alto (ej. Filtro de Spam [8], recomendaciones de inversión).[9, 17]

#### 2.2.4 Recall (Sensibilidad)

*   **Definición Matemática:** También conocido como Sensibilidad (Sensitivity) o Tasa de Verdaderos Positivos (TPR).[10, 18] Es la fracción de todos los casos *reales positivos* que el modelo identificó correctamente.[9, 10]
    $$Recall = \frac{TP}{TP + FN}$$
*   **Interpretación Conceptual:** "De todos los casos que *eran realmente* 'Positivos', ¿qué porcentaje *encontró* el modelo?".[10]
*   **Contexto de Uso:** Métrica crítica cuando el costo de un Falso Negativo (FN) es alto (ej. Diagnóstico médico, detección de fraude).[9]

### Sección 2.3 El Tradeoff Precisión-Recall y la Métrica F1

#### 2.3.1 El Tradeoff Dependiente del Umbral

La mayoría de los algoritmos de clasificación (ej. Regresión Logística, Redes Neuronales) no emiten una clase discreta (0 o 1), sino una *puntuación* o *probabilidad* continua (ej. 0.85). Se requiere un *umbral de decisión* (threshold) para convertir esta puntuación en una predicción de clase (ej. si > 0.5, predecir 'Positivo').[9, 17]

Las métricas de Precisión y Recall no son propiedades estáticas de un modelo; son funciones de este umbral de decisión.[9, 17] Existe un *tradeoff* inevitable entre ellas [19]:

*   **Si se aumenta el Umbral (ej. a 0.9):** El modelo se vuelve más "cauteloso" [19] y solo predice 'Positivo' si está extremadamente seguro.
    *   $FP$ disminuyen drásticamente (pocos negativos alcanzan el umbral). Esto *aumenta* la Precision.
    *   $FN$ aumentan (muchos positivos verdaderos no alcanzan el umbral). Esto *disminuye* el Recall.
*   **Si se disminuye el Umbral (ej. a 0.1):** El modelo se vuelve más "sensible" [19] y predice 'Positivo' con facilidad.
    *   $FN$ disminuyen (casi todos los positivos son "atrapados"). Esto *aumenta* el Recall.
    *   $FP$ aumentan (muchos negativos se "cuelan" por encima del umbral). Esto *disminuye* la Precision.

#### 2.3.2 F1-Score: La Media Armónica

Dado el tradeoff P-R, se necesita una métrica única que balancee ambas, especialmente útil en datasets desbalanceados donde el *Accuracy* falla.[9, 10]

*   **Definición Matemática:** El F1-Score es la *media armónica* de Precision y Recall.[9, 18]
    $$F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$$

#### 2.3.3 Justificación Estadística del Uso de la Media Armónica

El uso de la media armónica [20] (en lugar de una media aritmética simple) es una elección estadística deliberada y fundamental. La media armónica es la forma matemáticamente correcta de promediar *tasas* [21, 22] y su propiedad clave es que *penaliza fuertemente el desequilibrio extremo* entre los valores.[21, 23]

Analicemos por qué la media aritmética ($\frac{P+R}{2}$) es una métrica pobre:

*   **Modelo A (Desbalanceado, Malo):**
    *   Precision = 1.0 (perfecta)
    *   Recall = 0.02 (terrible)
    *   *Media Aritmética:* $\frac{1.0 + 0.02}{2} = 0.51$ (Esto sugiere falsamente que el modelo es "decente").
    *   *Media Armónica (F1):* $2 \cdot \frac{1.0 \cdot 0.02}{1.0 + 0.02} = \frac{0.04}{1.02} \approx 0.039$ (Esto refleja correctamente que el modelo es *malo*).

*   **Modelo B (Balanceado, Bueno):**
    *   Precision = 0.9
    *   Recall = 0.8
    *   *Media Aritmética:* $\frac{0.9 + 0.8}{2} = 0.85$
    *   *Media Armónica (F1):* $2 \cdot \frac{0.9 \cdot 0.8}{0.9 + 0.8} = \frac{1.44}{1.7} \approx 0.847$ (El valor es similar cuando P y R están balanceados).

La media aritmética puede ser alta incluso si uno de sus componentes es cercano a cero. La media armónica es arrastrada hacia el valor más bajo.[23] Por lo tanto, un F1-Score alto *garantiza* que tanto Precision como Recall tienen valores altos.[21]

**Tabla 2: Resumen de Métricas de Clasificación**

| Métrica | Fórmula Matemática | Pregunta Conceptual que Responde |
| :--- | :--- | :--- |
| **Accuracy** | $\frac{TP+TN}{Total}$ | ¿Qué fracción de *todas* las predicciones fue correcta? [9] |
| **Precision** | $\frac{TP}{TP+FP}$ | De lo que *predije* como positivo, ¿cuánto acerté? [10] |
| **Recall** | $\frac{TP}{TP+FN}$ | De lo que *era* positivo, ¿cuánto *encontré*? [10] |
| **F1-Score** | $2 \frac{Precision \cdot Recall}{Precision+Recall}$ | ¿Cuál es el balance (media armónica) entre Precision y Recall? [9] |

---

## III. Métricas de Evaluación para Problemas de Regresión

En regresión, la salida es un valor continuo. Las métricas evalúan la *magnitud* de la diferencia entre el valor real ($y_i$) y el valor predicho ($\hat{y}_i$). El error (o residual) se define como $e_i = y_i - \hat{y}_i$.

### Sección 3.1 Métricas Basadas en la Magnitud del Error

#### 3.1.1 Mean Absolute Error (MAE)

*   **Definición Matemática:** El promedio de las magnitudes absolutas de los errores. Esto corresponde a la *Pérdida L1*.[24]
    $$MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| = \frac{1}{n} \sum_{i=1}^{n} |e_i|$$
*   **Propiedades:**
    *   **Interpretabilidad:** El MAE se expresa en las *mismas unidades* que la variable objetivo $y$.[25, 26, 27] Si $y$ se mide en Dólares, el MAE es Dólares, representando el error promedio.[25]
    *   **Robustez:** Es *robusto* (menos sensible) a los *outliers*.[25, 26, 28, 29] Un error grande (outlier) contribuye de forma lineal (no cuadrática) al error total.

#### 3.1.2 Mean Squared Error (MSE)

*   **Definición Matemática:** El promedio de los errores al cuadrado. Esto corresponde a la *Pérdida L2*.[24, 30, 31]
    $$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} e_i^2$$
*   **Propiedades:**
    *   **Sensibilidad a Outliers:** Es *altamente sensible* a *outliers*.[28, 29, 31, 32, 33]
    *   **Penalización Cuadrática:** La naturaleza del cuadrado ($e_i^2$) significa que los errores grandes son penalizados *desproporcionadamente* más que los pequeños.[31, 34] Un error de 10 unidades contribuye 100 al MSE, mientras que un error de 2 contribuye 4.
    *   **Diferenciabilidad:** La función $e^2$ es suave y diferenciable en $e=0$, lo que la hace matemáticamente conveniente para la optimización (ej. descenso de gradiente).[28, 33]
    *   **Interpretabilidad:** Las unidades están *al cuadrado* (ej. Dólares$^2$), lo cual carece de interpretación física directa.[29, 30]

#### 3.1.3 Root Mean Squared Error (RMSE)

*   **Definición Matemática:** Es simplemente la raíz cuadrada del MSE, diseñada para resolver el problema de interpretabilidad de las unidades.[24, 35, 36, 37]
    $$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} = \sqrt{MSE}$$
*   **Propiedades:**
    *   **Interpretabilidad:** Resuelve el problema de unidades del MSE. RMSE se expresa en las *mismas unidades* que $y$, al igual que MAE.[27, 34, 37, 38]
    *   **Sensibilidad a Outliers:** Al ser una transformación monotónica de MSE, *conserva la misma alta sensibilidad* a los *outliers*.[29, 34]

#### 3.1.4 Análisis Comparativo: MAE vs. RMSE (MSE)

La elección entre MAE y RMSE (o MSE) no es trivial y va más allá de la simple robustez. Implica una elección fundamental sobre el *objetivo* del modelo y un *supuesto estadístico* sobre la distribución de los errores ($e_i$).[28, 39]

*   **Minimizar MSE (Pérdida L2):** Desde una perspectiva estadística, el valor que minimiza la suma de errores al cuadrado es la *media* de la distribución.
    *   *Consecuencia:* Si los errores siguen (o se asume que siguen) una *distribución Gaussiana* (Normal), MSE (y RMSE) es la métrica óptima.[39] El modelo se verá fuertemente penalizado por los *outliers* y se esforzará por predecirlos, ya que estos tienen un gran impacto en la media.[28, 32]
*   **Minimizar MAE (Pérdida L1):** El valor que minimiza la suma de diferencias absolutas es la *mediana* de la distribución.
    *   *Consecuencia:* Si los errores siguen (o se asume que siguen) una *distribución Laplaciana* (con colas más pesadas que la Gaussiana), MAE es la métrica óptima.[39] El modelo es *robusto* a los *outliers* [25], ya que prefiere ajustarse a la mediana de la tendencia central e ignorar los valores extremos.[28]

En resumen, se debe usar RMSE [27] si los errores grandes son *particularmente indeseables* y deben ser penalizados fuertemente. Se debe usar MAE [27] si los *outliers* se consideran ruido que el modelo debe *ignorar*.[26]

**Tabla 3: Comparativa de Métricas de Error en Regresión**

| Métrica | Fórmula Matemática | Unidades (relativas a $y$) | Sensibilidad a Outliers | Propiedad Estadística |
| :--- | :--- | :--- | :--- | :--- |
| **MAE** | $\frac{1}{n} \sum |y_i - \hat{y}_i|$ | $y$ [26] | Baja (Robusta) [25, 28] | Asociada a la *Mediana* del error [28, 39] |
| **MSE** | $\frac{1}{n} \sum (y_i - \hat{y}_i)^2$ | $y^2$ [29, 30] | Alta [28, 31, 32] | Asociada a la *Media* del error [28, 39] |
| **RMSE** | $\sqrt{MSE}$ | $y$ [37, 38] | Alta [34] | Asociada a la *Media* del error [39] |

### Sección 3.2 Coeficiente de Determinación (R²)

#### 3.2.1 Interpretación Conceptual

A diferencia de MAE/MSE/RMSE (que miden el error *absoluto* en unidades de $y$), el Coeficiente de Determinación (R²) es una métrica *relativa* y *adimensional*.[40, 41] Su valor está típicamente acotado entre 0 y 1 [41, 42] (aunque puede ser negativo para modelos peores que el promedio).

*   **Interpretación:** R² mide la *proporción de la varianza* en la variable dependiente ($y$) que es *predecible* (o "explicada") por las variables independientes ($X$) a través del modelo.[40, 41, 43]
*   Un R² = 0.75 significa que el 75% de la variabilidad en $y$ (respecto a su media) es explicada por el modelo, y el 25% restante es varianza residual (error).[41]

#### 3.2.2 Derivación Matemática Formal (Descomposición de la Varianza)

La fórmula de R² no es arbitraria; se deriva directamente de la *descomposición de la varianza* en el análisis de regresión.[44, 45]

*   **Paso 1: Suma Total de Cuadrados (TSS):** Mide la varianza total de $y$. Esto es equivalente al error de un "modelo base" (ingenuo) que siempre predice la media $\bar{y}$.
    $$TSS = \sum_{i=1}^n (y_i - \bar{y})^2$$.[45]
*   **Paso 2: Suma de Cuadrados Residuales (RSS):** Mide el error *inexplicado* por nuestro modelo $f$.[45] (También conocido como SSE, Sum of Squared Errors [46, 47]).
    $$RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$.[45]
*   **Paso 3: Descomposición (para OLS):** Se puede demostrar que la varianza total se descompone en la varianza explicada y la no explicada [44, 45]:
    $$TSS = ESS + RSS$$
    (Donde $ESS = \sum (\hat{y}_i - \bar{y})^2$ es la Suma de Cuadrados Explicada).[45]
*   **Paso 4: Definición de R²:** R² se define conceptualmente como la proporción de varianza explicada.[44, 45]
    $$R^2 = \frac{ESS}{TSS}$$
*   **Paso 5: Fórmula Práctica:** Sustituyendo $ESS = TSS - RSS$:
    $$R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}$$.[40, 45, 46, 47]

Esta fórmula compara el error de nuestro modelo (RSS) con el error del modelo base (TSS).[40] Si $RSS=0$ (ajuste perfecto), $R^2=1$. Si $RSS=TSS$ (nuestro modelo no es mejor que predecir la media), $R^2=0$.[47]

#### 3.2.3 Limitaciones Críticas y Malas Interpretaciones de R²

R² es una de las métricas más frecuentemente malinterpretadas en la práctica.[48, 49]

*   **Limitación 1: R² no mide la "bondad de ajuste"**.[48] Un R² alto no significa que el modelo sea correcto. Un modelo con una forma funcional incorrecta (ej. lineal cuando la relación es cuadrática) puede tener un R² alto.
*   **Limitación 2: R² no mide el error de predicción**.[48] Un R² de 0.90 suena bien, pero si la varianza (TSS) de $y$ es masiva, el 10% de error restante (RMSE) puede ser inaceptablemente alto en términos prácticos.
*   **Limitación 3: Sensibilidad al Overfitting**.[50] R² *siempre aumenta* (o, en el peor caso, se mantiene) cuando se añade una nueva variable predictora al modelo, *incluso si esa variable es ruido aleatorio*.[50] El modelo usa esta variable para explicar una porción minúscula del ruido en la muestra de entrenamiento.
    *   *Consecuencia:* Esto fomenta el sobreajuste.[49, 50] (Nota: El *Adjusted R²* [45, 46] fue creado para penalizar la inclusión de predictores $p$ irrelevantes, pero R² estándar no lo hace).
*   **Limitación 4: Invalidez en Comparaciones**.[51] R² no puede usarse para comparar modelos donde la variable $y$ ha sido transformada (ej. $R^2$ de un modelo que predice $y$ no es comparable al $R^2$ de un modelo que predice $\log(y)$), ya que el TSS es diferente.
*   **Limitación 5: Sensibilidad a Outliers**.[50] Al estar basado en *cuadrados* (TSS y RSS), el valor de R² puede ser fuertemente influenciado por unos pocos *outliers*.
*   **Limitación 6: Correlación no implica causalidad**.[43, 50] Un R² alto no dice nada sobre la relación causal entre $X$ e $Y$.

---

## IV. Metodologías de Validación para la Estimación del Error de Generalización

Como se estableció en la Sección I, el error de entrenamiento ($R_{emp}$) es un estimador sesgado. Necesitamos técnicas de muestreo para obtener un estimador más fiable del error de generalización ($R(f)$).

### Sección 4.1 El Método Holdout (Train/Test Split)

#### 4.1.1 Procedimiento Conceptual

Es la técnica de validación más simple.[1] El conjunto de datos original $D$ se divide *una vez* de forma aleatoria en dos subconjuntos [1]:

1.  **Conjunto de Entrenamiento (Training Set):** La porción más grande (ej. 70%, 80%), usada para entrenar el modelo (aprender los parámetros).[1, 52]
2.  **Conjunto de Prueba (Test Set):** La porción restante (ej. 30%, 20%), usada para evaluar el modelo entrenado en datos *no vistos* y estimar el error de generalización.[1, 52, 53]

#### 4.1.2 El Imperativo del Conjunto de Validación (Train/Validation/Test Split)

El uso del *Test Set* para tomar decisiones de modelado (ej. seleccionar hiperparámetros como `max_depth` [1], o elegir qué variables incluir) es un error metodológico grave.[3, 53]

Si se usa el *Test Set* repetidamente para "ajustar" el modelo ("Tweak model" en [53]), el modelo comienza a *sobreajustarse* a la información específica de ese *Test Set*.[3, 53] El *Test Set* "se desgasta" ("wear out") [53] y deja de ser una medida insesgada de generalización; la estimación del error reportada será *optimistamente sesgada*.

**Protocolo Correcto (3 Particiones) [52, 53]:**
1.  **Training Set (ej. 60%):** Para entrenar los modelos (aprender parámetros).
2.  **Validation Set (ej. 20%):** Para ajustar hiperparámetros (ej. comparar $k=3$ vs $k=5$) y realizar la selección del modelo.[52, 53]
3.  **Test Set (Holdout) (ej. 20%):** Se mantiene "bloqueado" y aislado. Se usa *una sola vez* al final del proyecto para reportar el error de generalización *insesgado* del modelo final seleccionado.[3, 53]

#### 4.1.3 Limitación Fundamental: Alta Varianza del Estimador

La debilidad principal del método Holdout es que la estimación del error (medida en el *validation* o *test set*) depende *fuertemente* de la partición aleatoria específica que se realizó.[54, 55] Si, por mala suerte, la partición de prueba contiene muestras "difíciles" u *outliers*, el error estimado será *pesimista*. Si contiene muestras "fáciles", será *optimista*.[56]

La estimación del error del método Holdout tiene una *alta varianza*.[56] No es una estimación robusta, especialmente en datasets pequeños.[56]

### Sección 4.2 Validación Cruzada K-Fold (K-Fold Cross-Validation)

K-Fold CV es un procedimiento de *resampling* [2, 57] diseñado para mitigar la alta varianza del método Holdout, proporcionando una estimación del error más robusta.[56, 58]

#### 4.2.1 Descripción Algorítmica

1.  Barajar (Shuffle) aleatoriamente el conjunto de datos $D$.
2.  Particionar $D$ en $k$ subconjuntos (folds) de tamaño (aproximadamente) igual: $D_1, D_2,..., D_k$.[57, 59]
3.  Para $i$ desde $1$ hasta $k$:
    a.  Usar $D_i$ como el *fold de validación* (hold-out fold).[57, 60]
    b.  Usar los $k-1$ folds restantes ($D - D_i$) como el *conjunto de entrenamiento*.[57, 60]
    c.  Entrenar un modelo $M_i$ en $D - D_i$ y calcular su error $E_i$ en $D_i$.
4.  Descartar los $k$ modelos entrenados ($M_1...M_k$) [57] (el objetivo es evaluar el *proceso de modelado*, no los modelos individuales).
5.  El estimador del error de generalización ($E_{CV}$) es el promedio de los $k$ errores:
    $$E_{CV} = \frac{1}{k} \sum_{i=1}^k E_i$$.[59, 60]

Valores comunes son $k=5$ o $k=10$.[59, 61]

#### 4.2.2 Ventajas sobre Holdout

*   **Estimación Robusta (Baja Varianza):** Al promediar $k$ estimaciones de error de $k$ particiones diferentes, la estimación final $E_{CV}$ es mucho más estable (menor varianza) y menos dependiente de una única partición aleatoria.[2, 58, 61, 62]
*   **Uso Eficiente de los Datos:** *Todas* las muestras del dataset se utilizan tanto para entrenamiento como para validación (en diferentes iteraciones).[58, 59, 61] Esto es una ventaja crítica en datasets pequeños donde no se puede "desperdiciar" datos en un gran *test set*.[56, 59]

### Sección 4.3 Variantes Esenciales de la Validación Cruzada

#### 4.3.1 Stratified K-Fold (CV Estratificada)

El K-Fold estándar (aleatorio) [57] falla en problemas de clasificación desbalanceada.[63]

Si un dataset es 99% Clase A y 1% Clase B, el *muestreo aleatorio* para crear los $k$ folds no garantiza que esta proporción se mantenga en cada fold.[63] Es estadísticamente posible (y probable en datasets pequeños) que un fold (el fold de validación) termine conteniendo *cero* muestras de la Clase B.[63] En esa iteración $i$, el modelo $M_i$ será evaluado en un fold sin la clase minoritaria. Métricas como Recall o F1 no podrán calcularse o serán 0, sesgando el promedio final $E_{CV}$.

**Definición de Stratified K-Fold:** Es una modificación del Paso 2 del algoritmo. La partición en $k$ folds *no* es aleatoria, sino *estratificada*.[64, 65, 66] El algoritmo asegura que la distribución de clases (la probabilidad *a priori* $P(y)$) en *cada uno* de los $k$ folds sea (lo más cercanamente posible) idéntica a la distribución de clases en el dataset completo $D$.[63, 64] Esta es la metodología *mandatoria* para la validación en clasificación desbalanceada.

#### 4.3.2 Leave-One-Out Cross-Validation (LOOCV)

*   **Definición:** Es el caso extremo de K-Fold CV donde $k$ es igual al número total de muestras, $N$ ($k=N$).[59]
*   **Algoritmo [67]:**
    1.  Para $i$ desde $1$ hasta $N$:
        a.  Entrenar el modelo $M_i$ en *todas* las muestras *excepto* la muestra $i$ (tamaño de entrenamiento $N-1$).[59]
        b.  Evaluar (testear) el modelo $M_i$ en la *única* muestra $i$ que se omitió.[59, 67]
    2.  El error $E_{LOOCV}$ es el promedio de los $N$ errores.
*   **Propiedad:** El proceso es *determinista* (no hay aleatoriedad en las particiones, ya que solo hay una forma de omitir un punto).[68]

---

## V. Análisis Teórico Comparativo de las Técnicas de Validación

La selección de una técnica de validación (Holdout, K-Fold, LOOCV) implica un *tradeoff* de sesgo-varianza en la *estimación del error de generalización*. Es crucial no confundir esto con el sesgo-varianza del *modelo* en sí.

### Sección 5.1 Análisis del Sesgo (Precisión del Estimador)

El *sesgo* del estimador de error mide qué tan lejos está el error *estimado* ($E_{CV}$) del *verdadero error de generalización* ($R(f)$) que tendría un modelo entrenado en *todos* los datos ($N$).[61]

Todos los métodos de validación (Holdout, K-Fold) entrenan modelos en *subconjuntos* de los datos (ej. 70% de $N$, o $(k-1)/k$ de $N$). Los modelos entrenados con *menos datos* son, en promedio, *peores* (tienen mayor error) que el modelo final entrenado con el 100% de los datos.[59]

*Consecuencia:* El error estimado ($E_{CV}$) es *pesimista*, es decir, *sobreestima* el verdadero error del modelo final.

*   **Holdout (ej. 70/30):** Entrena en el 70% de los datos. El tamaño del entrenamiento es significativamente menor que $N$. El sesgo pesimista es *alto*.
*   **K-Fold (k=10):** Entrena en el 90% de los datos. El sesgo pesimista es *pequeño*.[69]
*   **LOOCV:** Entrena en $N-1$ datos (casi el 100%). El modelo es casi idéntico al modelo final. El sesgo es *casi cero*.[68] LOOCV es un estimador casi insesgado del error de generalización.

### Sección 5.2 Análisis de la Varianza (Estabilidad del Estimador)

La *varianza* del estimador de error mide qué tan *sensible* es la estimación ($E_{CV}$) a la composición del dataset. Si repitiéramos el proceso en un dataset $D'$ diferente (muestreado de la misma $P(x,y)$), ¿cuánto cambiaría $E_{CV}$?.[61]

*   **Holdout:** Varianza *alta*.[56] La estimación depende totalmente de una única partición aleatoria.
*   **K-Fold (k=5, k=10):** Varianza *baja*.[61, 69] El promedio sobre $k$ folds estabiliza la estimación.[62]
*   **LOOCV:** Varianza *alta*.[59, 67, 69]

**La Paradoja de LOOCV (Bajo Sesgo, Alta Varianza):**
Esto es un resultado teórico fundamental y contraintuitivo. ¿Cómo puede LOOCV (que promedia $N$ resultados) tener alta varianza?.[59, 69]

La razón es la *correlación*. En LOOCV, los $N$ modelos entrenados son *casi idénticos*. El modelo $M_1$ (entrenado en $D - \{d_1\}$) y el modelo $M_2$ (entrenado en $D - \{d_2\}$) comparten $N-2$ de sus $N-1$ puntos de entrenamiento. Por lo tanto, los $N$ modelos están *altamente correlacionados*.[59]

Estadísticamente, la varianza del promedio de variables altamente correlacionadas *no* se reduce significativamente. (La fórmula $\frac{\sigma^2}{N}$ para la varianza de la media solo aplica si las variables son *independientes*). LOOCV promedia $N$ estimaciones de error que están muy correlacionadas, resultando en un estimador final $E_{LOOCV}$ con alta varianza.[59, 69]

### Sección 5.3 Análisis del Costo Computacional

*   **Holdout:** Costo $O(1)$. Se entrena 1 modelo.
*   **K-Fold:** Costo $O(k)$. Se entrenan $k$ modelos.[59]
*   **LOOCV:** Costo $O(N)$. Se entrenan $N$ modelos.[59, 67, 70]

Consecuentemente, LOOCV es computacionalmente *inviable* para datasets grandes (ej. $N > 10,000$) o para modelos cuyo entrenamiento es costoso (ej. Redes Neuronales Profundas).[70]

### Sección 5.4 Síntesis y Recomendación Práctica

El *tradeoff* en la selección de la técnica de validación es triple: Sesgo vs. Varianza vs. Costo.

*   **Holdout:** Rápido (O(1)), pero alto sesgo y alta varianza. No recomendado, excepto para datasets masivos donde el *test set* es suficientemente grande.
*   **LOOCV:** Sesgo casi nulo, pero alta varianza y costo computacional O(N). No recomendado, excepto para datasets muy pequeños donde maximizar los datos de entrenamiento es la única prioridad.[68, 70]
*   **K-Fold (k=5 o k=10):** Es el estándar empírico y teórico.[59, 61, 69] Proporciona el mejor *compromiso* [69]:
    *   *Bajo sesgo* (entrena en 80%-90% de los datos).
    *   *Baja varianza* (promedia sobre $k$ folds suficientemente diferentes).
    *   *Costo computacional manejable* (O(k)).

**Tabla 4: Comparativa Teórica de Técnicas de Validación**

| Técnica | Sesgo del Estimador de Error | Varianza del Estimador de Error | Costo Computacional | Tamaño del Set de Entrenamiento |
| :--- | :--- | :--- | :--- | :--- |
| **Holdout** | Alto (Pesimista) | Alta (Inestable) [56] | $O(1)$ (Bajo) | (ej. 70% N) |
| **K-Fold (k=5, 10)** | Bajo (Pesimista) [69] | Baja (Estable) [61, 69] | $O(k)$ (Moderado) [59] | $\frac{k-1}{k} N$ |
| **LOOCV (k=N)** | Muy Bajo (Casi Insesgado) [68] | Alta (Inestable) [59, 69] | $O(N)$ (Alto) [67, 70] | $N-1$ |
