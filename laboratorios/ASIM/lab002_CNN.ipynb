{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import utils\n",
    "from torch import optim\n",
    "from torch import device\n",
    "from torch import inference_mode\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torchmetrics import ConfusionMatrix\n",
    "import mlxtend\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import numpy\n",
    "from torchvision.transforms.v2 import (\n",
    "    ConvertImageDtype,\n",
    "    Normalize,\n",
    "    Resize,\n",
    "    CenterCrop,\n",
    "    ToTensor,\n",
    "    ToImage,\n",
    "    Compose\n",
    ")\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "1. GPU 0: NVIDIA GeForce MX110\n",
      "GPU selection: NVIDIA GeForce MX110\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")\n",
    "for i in range(num_gpus):\n",
    "    print(f\"{i+1}. GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "device = 0  # \"Select the index of the GPU you wish to use\"\n",
    "torch.cuda.set_device(device)\n",
    "print(f\"GPU selection: {torch.cuda.get_device_name(device)}\")\n",
    "\n",
    "device1 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.local/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transformacion = Compose([\n",
    "    ToTensor(), \n",
    "    Normalize(mean=[0.5], std=[0.5])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://zenodo.org/records/10519652/files/pathmnist.npz?download=1 to /home/pablo/.medmnist/pathmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205615438/205615438 [00:20<00:00, 10059790.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/pablo/.medmnist/pathmnist.npz\n",
      "Using downloaded and verified file: /home/pablo/.medmnist/pathmnist.npz\n"
     ]
    }
   ],
   "source": [
    "data_flag = \"pathmnist\"\n",
    "info = INFO[data_flag]\n",
    "DataClass = getattr(medmnist, info[\"python_class\"])\n",
    "\n",
    "# Load the training and testing datasets\n",
    "train_data = DataClass(split=\"train\", transform=transformacion, download=True)\n",
    "val_data = DataClass(split=\"val\", transform=transformacion, download=True)\n",
    "test_data = DataClass(split=\"test\", transform=transformacion, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:\n",
      " tensor([[[0.7255, 0.7176, 0.7255,  ..., 0.7255, 0.7176, 0.7333],\n",
      "         [0.7098, 0.7255, 0.7176,  ..., 0.5451, 0.5059, 0.4902],\n",
      "         [0.7255, 0.7255, 0.7176,  ..., 0.6314, 0.6235, 0.6392],\n",
      "         ...,\n",
      "         [0.7098, 0.7020, 0.7333,  ..., 0.7333, 0.7255, 0.7333],\n",
      "         [0.6706, 0.7020, 0.7333,  ..., 0.7333, 0.7333, 0.7333],\n",
      "         [0.6863, 0.7255, 0.7333,  ..., 0.7255, 0.7333, 0.7412]],\n",
      "\n",
      "        [[0.6314, 0.6235, 0.6235,  ..., 0.6314, 0.6235, 0.6314],\n",
      "         [0.6157, 0.6235, 0.6157,  ..., 0.3882, 0.3490, 0.3176],\n",
      "         [0.6314, 0.6235, 0.6078,  ..., 0.4980, 0.5059, 0.5216],\n",
      "         ...,\n",
      "         [0.6078, 0.5765, 0.6314,  ..., 0.6314, 0.6314, 0.6392],\n",
      "         [0.5059, 0.5686, 0.6314,  ..., 0.6314, 0.6392, 0.6314],\n",
      "         [0.5294, 0.6235, 0.6314,  ..., 0.6314, 0.6314, 0.6392]],\n",
      "\n",
      "        [[0.7804, 0.7804, 0.7804,  ..., 0.7804, 0.7804, 0.7804],\n",
      "         [0.7725, 0.7725, 0.7725,  ..., 0.5843, 0.5451, 0.5294],\n",
      "         [0.7725, 0.7725, 0.7647,  ..., 0.6706, 0.6706, 0.6941],\n",
      "         ...,\n",
      "         [0.7647, 0.7412, 0.7804,  ..., 0.7804, 0.7804, 0.7804],\n",
      "         [0.7098, 0.7412, 0.7804,  ..., 0.7804, 0.7804, 0.7804],\n",
      "         [0.7255, 0.7725, 0.7804,  ..., 0.7804, 0.7804, 0.7882]]])\n",
      "Label:\n",
      " [0]\n",
      "Image shape: torch.Size([3, 28, 28])\n",
      "Label: [0]\n"
     ]
    }
   ],
   "source": [
    "# check data properties\n",
    "img = train_data[0][0]\n",
    "label = train_data[0][1]\n",
    "\n",
    "print(f\"Image:\\n {img}\")\n",
    "print(f\"Label:\\n {label}\")\n",
    "\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of channels: 3\n",
      "number of classes: 9\n",
      "class names: {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n"
     ]
    }
   ],
   "source": [
    "# Number of image channels\n",
    "n_channels = info[\"n_channels\"]\n",
    "print(f\"number of channels: {n_channels}\")\n",
    "\n",
    "# Number of classes\n",
    "n_classes = len(info[\"label\"])\n",
    "print(f\"number of classes: {n_classes}\")\n",
    "\n",
    "# Get the class names from the dataset\n",
    "class_names = info[\"label\"]\n",
    "print(f\"class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.6313726..0.84313726].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.372549..0.85882354].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    img = train_data[i][0]\n",
    "    label = train_data[i][1]\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "    plt.title(label)\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data into dataloader form\n",
    "BATCH_SIZE = 128\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7fd88dd53cd0>, <torch.utils.data.dataloader.DataLoader object at 0x7fd88dd53490>)\n",
      "Length of train dataloader: 704 batches of 128\n",
      "Length of test dataloader: 57 batches of 128\n",
      "Length of val dataloader: 79 batches of 128\n"
     ]
    }
   ],
   "source": [
    "# check dataloader\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of val dataloader: {len(val_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training loop functions\n",
    "def train_step(\n",
    "    model: torch.nn.Module,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    accuracy_fn,\n",
    "    device: torch.device = device,\n",
    "):\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.to(device)\n",
    "\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # need to change target shape for this medmnist data\n",
    "        y = y.squeeze().long()\n",
    "\n",
    "        # Send data to selected device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. loss and accuracy\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate loss and accuracy per epoch\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    accuracy_fn,\n",
    "    device: torch.device = device,\n",
    "):\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()  # eval mode for testing\n",
    "    with torch.inference_mode():  # Inference context manager\n",
    "        for X, y in data_loader:\n",
    "            # need to change target shape for this medmnist data\n",
    "            y = y.squeeze().long()\n",
    "\n",
    "            # Send data to selected device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "\n",
    "            # 2. Calculate loss and accuracy\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "\n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_func(\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    accuracy_fn,\n",
    "    device: torch.device = device,\n",
    "):\n",
    "\n",
    "    eval_loss, eval_acc = 0, 0\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    y_preds = []\n",
    "    y_targets = []\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in tqdm(enumerate(data_loader)):\n",
    "            # need to change target shape for this medmnist data\n",
    "            y = y.squeeze().long()\n",
    "\n",
    "            # Send data to selected device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            eval_pred = model(X)\n",
    "\n",
    "            # Find loss and accuracy\n",
    "            eval_loss += loss_fn(eval_pred, y)\n",
    "            eval_acc += accuracy_fn(y_true=y, y_pred=eval_pred.argmax(dim=1))\n",
    "\n",
    "            # Add prediction and target labels to list\n",
    "            eval_labels = torch.argmax(torch.softmax(eval_pred, dim=1), dim=1)\n",
    "            y_preds.append(eval_labels)\n",
    "            y_targets.append(y)\n",
    "\n",
    "        # Scale loss and acc\n",
    "        eval_loss /= len(data_loader)\n",
    "        eval_acc /= len(data_loader)\n",
    "\n",
    "        # Put predictions on CPU for evaluation\n",
    "        y_preds = torch.cat(y_preds).cpu()\n",
    "        y_targets = torch.cat(y_targets).cpu()\n",
    "\n",
    "        return {\n",
    "            \"model_name\": model.__class__.__name__,\n",
    "            \"loss\": eval_loss.item(),\n",
    "            \"accuracy\": eval_acc,\n",
    "            \"predictions\": y_preds,\n",
    "            \"targets\": y_targets,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnn(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class cnn(torch.nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape, out_channels=hidden_units, kernel_size=3\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_units),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units, out_channels=hidden_units, kernel_size=3\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units, out_channels=hidden_units * 4, kernel_size=3\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_units * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units * 4,\n",
    "                out_channels=hidden_units * 4,\n",
    "                kernel_size=3,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_units * 4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units * 4,\n",
    "                out_channels=hidden_units * 4,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_units * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_units * 4 * 4 * 4, hidden_units * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units * 8, hidden_units * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units * 8, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define Model\n",
    "model = cnn(input_shape=n_channels, hidden_units=16, output_shape=n_classes).to(device)\n",
    "\n",
    "\n",
    "# Setup loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# View Model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debbfb67bc1244a4b0b152f46eff793e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m\n\u001b[1;32m     16\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_step(\n\u001b[1;32m     17\u001b[0m     data_loader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice1,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(\n\u001b[1;32m     26\u001b[0m     data_loader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice1,\n\u001b[1;32m     31\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     34\u001b[0m     iteration_loss_list\u001b[38;5;241m.\u001b[39mappend(train_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     35\u001b[0m     iteration_accuracy_list\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/medmnist/dataset.py:138\u001b[0m, in \u001b[0;36mMedMNIST2D.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03mreturn: (without transform/target_transofrm)\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    img: PIL.Image\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    target: np.array of `L` (L=1 for single-label)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs[index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_rgb:\n\u001b[1;32m    141\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:3304\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3301\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrides\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires either tobytes() or tostring()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 3304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:3206\u001b[0m, in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3203\u001b[0m         im\u001b[38;5;241m.\u001b[39mreadonly \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3204\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m im\n\u001b[0;32m-> 3206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:3138\u001b[0m, in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3135\u001b[0m _check_size(size)\n\u001b[1;32m   3137\u001b[0m im \u001b[38;5;241m=\u001b[39m new(mode, size)\n\u001b[0;32m-> 3138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3139\u001b[0m     decoder_args: Any \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m   3140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(decoder_args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(decoder_args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   3141\u001b[0m         \u001b[38;5;66;03m# may pass tuple instead of argument list\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:559\u001b[0m, in \u001b[0;36mImage.height\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwidth\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mheight\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    563\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msize\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Measure Time\n",
    "\n",
    "train_time_start_model = timer()\n",
    "\n",
    "iteration_loss_list = []\n",
    "iteration_accuracy_list = []\n",
    "\n",
    "# set parameters\n",
    "epochs = 10\n",
    "best_loss = 10\n",
    "\n",
    "# call train and test function\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_acc = train_step(\n",
    "        data_loader=train_dataloader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device1,\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc = test_step(\n",
    "        data_loader=test_dataloader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device1,\n",
    "    )\n",
    "\n",
    "    for iteration, (x, y) in enumerate(train_dataloader):\n",
    "        iteration_loss_list.append(train_loss.item())\n",
    "        iteration_accuracy_list.append(train_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch} | Training loss: {train_loss:.3f} | Training acc: {train_acc:.2f} | Test loss: {test_loss:.3f} | Test acc: {test_acc:.2f}\"\n",
    "    )\n",
    "\n",
    "    # save best model instance\n",
    "\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        print(f\"Saving best model for epoch: {epoch}\")\n",
    "        torch.save(obj=model.state_dict(), f=\"./model.pth\")\n",
    "\n",
    "\n",
    "train_time_end_model = timer()\n",
    "total_train_time_model = print_train_time(\n",
    "    start=train_time_start_model, end=train_time_end_model, device=device1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "loaded_model = cnn(input_shape=n_channels, hidden_units=16, output_shape=n_classes).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "loaded_model.load_state_dict(torch.load(f=\"./model.pth\"))\n",
    "\n",
    "# get results\n",
    "model_results = eval_func(\n",
    "    data_loader=val_dataloader,\n",
    "    model=loaded_model,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_fn,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Model predictions and true targets\n",
    "y_targets = model_results[\"targets\"]\n",
    "y_preds = model_results[\"predictions\"]\n",
    "\n",
    "# Setup confusion matrix\n",
    "confmat = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_names))\n",
    "confmat_tensor = confmat(preds=y_preds, target=y_targets)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fix, ax = plot_confusion_matrix(\n",
    "    conf_mat=confmat_tensor.numpy(), class_names=class_names, figsize=(10, 7)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Model predictions and true targets\n",
    "y_targets = model_results[\"targets\"]\n",
    "y_preds = model_results[\"predictions\"]\n",
    "\n",
    "# Setup confusion matrix\n",
    "confmat = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_names))\n",
    "confmat_tensor = confmat(preds=y_preds, target=y_targets)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fix, ax = plot_confusion_matrix(\n",
    "    conf_mat=confmat_tensor.numpy(), class_names=class_names, figsize=(10, 7)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
