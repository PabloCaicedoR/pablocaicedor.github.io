{
  "hash": "f081a03546588c012826c89aa68f31cc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Descenso de Gradiente en Ciencias Biomédicas\"\nsubtitle: \"Regresión lineal múltiple y regresión logística\"\nauthor: \"PhD. Pablo Eduardo Caicedo Rodríguez\"\ndate: last-modified\nlang: es\nformat:\n  revealjs:\n    code-tools: true\n    code-overflow: wrap\n    code-line-numbers: true\n    code-copy: true\n    fig-align: center\n    self-contained: true\n    theme:\n      - simple\n      - ../../recursos/estilos/metropolis.scss\n    slide-number: true\n    preview-links: auto\n    logo: ../../recursos/imagenes/generales/Escuela_Rosario_logo.png\n    css: ../../recursos/estilos/styles_pres.scss\n    footer: <https://pablocaicedor.github.io/>\n    transition: fade\n    progress: true\n    scrollable: true\n    hash: true\n  beamer:\n    slide-level: 2\n    incremental: false\n    aspectratio: 32\n    navigation: horizontal\n    theme: CambridgeUS\n    header-includes:\n      \\makeatletter\n      \\expandafter\\let\\csname figure*\\endcsname\\figure\n      \\expandafter\\let\\csname endfigure*\\endcsname\\endfigure\n      \\expandafter\\let\\csname table*\\endcsname\\table\n      \\expandafter\\let\\csname endtable*\\endcsname\\endtable\nexecute:\n  echo: false\n  warning: false\n  freeze: auto\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n<!-- TODO: Cambiar chunks por tabset quarto. Un tab mostrando resultado y otro tab mostrando el código -->\n\n## Objetivos de la sesión\n\n- Entender el **principio del descenso de gradiente (GD)** como método de optimización.\n- Aplicar GD a **regresión lineal múltiple** (objetivo continuo) y **regresión logística** (clasificación 0/1).\n- Analizar decisiones de ingeniería: **tasa de aprendizaje**, **escalado/normalización**, **batch vs. mini-batch vs. SGD**.\n- Interpretar resultados en **contextos biomédicos** (diagnóstico, pronóstico y evaluación de riesgo).\n\n::: notes\nEstructura sugerida: 4 bloques de ~45+45+45+30 min. Actividades cortas para mantener la atención.\n:::\n\n---\n\n## Agenda\n\n1. Fundamentos de GD y funciones de costo\n2. Caso 1: **Regresión lineal múltiple** con GD\n3. Caso 2: **Regresión logística** con GD\n4. Buenas prácticas, diagnósticos y discusión aplicada\n\n---\n\n# 1. Fundamentos de descenso de gradiente\n\n## Motivación biomédica\n\n- Ajustar parámetros de un **modelo fisiológico** o un **predictor clínico** con datos reales.\n- Ejemplos típicos:\n  - Estimar **gasto energético** a partir de IMC, edad y FC.\n  - Estimar **edad vascular** con variables de laboratorio.\n  - **Clasificar** presencia/ausencia de una condición (0/1) con biomarcadores.\n\n::: notes\nEnfatizar que el GD es base de la mayoría de métodos de AA actuales, incluidas redes neuronales.\n:::\n\n---\n\n## Regresión Lineal\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](v2Lect002_AlgoritmoDescensoGradiente_files/figure-beamer/unnamed-chunk-2-1.pdf)\n:::\n:::\n\n\n## Regresión Lineal\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](v2Lect002_AlgoritmoDescensoGradiente_files/figure-beamer/unnamed-chunk-3-3.pdf)\n:::\n:::\n\n\n\n## Función de costo (error)\n\n:::columns\n:::{.column width=\"45%\"}\n**Regresión (continuo):**\n$$\nJ(\\mathbf{w}) \\;=\\; \\frac{1}{2m}\\sum_{i=1}^{m}\\left(y_j - \\hat{y}_j\\right)^2\n$$\ndonde $\\hat{y}_j = \\mathbf{w}^\\top \\mathbf{x}_j$.\n:::\n:::{.column .smaller width=\"45%\"}\n**Clasificación binaria:**\n**Entropía cruzada** (log-loss):\n$$\nJ(\\mathbf{w}) \\;=\\; -\\frac{1}{m}\\sum_{i=1}^{m}\\left[\\,y_i\\log\\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i)\\,\\right]\n$$\ndonde $\\hat{p}_i=\\sigma(\\mathbf{w}^\\top \\mathbf{x}_i)=\\frac{1}{1+e^{-\\mathbf{w}^\\top\\mathbf{x}_i}}$.\n:::\n:::\n\n::: notes\nConectar con la interpretación probabilística en logística y con el MSE en lineal.\n:::\n\n---\n\n## Función de costo (error)\n::: {.callout-important title=\"... Para el caso de ejemplo (un modelo tipo lineal)\"}\n\n\n$$\n\\mathbf{w_{j}}^\\top \\;=\\; \\left[w_{j,1}, w_{j,0}\\right]\n$$\n\n$$\nsalary_i = x_i\n$$\n\n$$\n\\hat{y}_j  \\;=\\; w_{j,1}*x_j + w_{j,0}\n$$\n\n$$\nJ(\\mathbf{w}) \\;=\\; \\frac{1}{2m}\\sum_{i=1}^{m}\\left(y_j - w_{j,1}*x_i - w_{j,0}\\right)^2\n$$\n\n:::\n\n---\n\n## Idea central del GD\n\n- Partimos de $\\mathbf{w}_{(0)}$ (p. ej., aleatorio).\n- Iteramos:\n$$\n\\mathbf{w}_{(t+1)} \\;=\\; \\mathbf{w}_{(t)} \\;-\\; \\alpha\\, \\nabla_{\\mathbf{w}} J(\\mathbf{w}_{(t)})\n$$\n- $\\alpha$ = **tasa de aprendizaje**: grande → rápido pero inestable; pequeña → estable pero lento.\n- Convergencia: buscamos $\\nabla J \\approx \\mathbf{0}$.\n\n**Visualización conceptual:** “valle” del error y trayectoria en zig-zag hacia el mínimo.\n\n---\n\n## Algoritmo de Descenso de Gradiente\n\n:::{.small_font}\n\n::: {.callout-note title=\"Para la estimación de $w_{j,0}$\" collapsible=\"false\"}\n\n$$\nw_{j+1,0} = w_{j,0} - \\alpha \\frac{\\partial J}{\\partial w_{j,0}}\n$$\n\n$$\n\\frac{\\partial J}{\\partial w_{j,0}} =\n\\frac{\\partial}{\\partial w_{j,0}}\n\\left(\n\\frac{1}{2m} \\sum_{i=1}^{m}\n\\left( y_i - w_{j,1} x_i - w_{j,0} \\right)^2\n\\right)\n$$\n\n$$\n\\frac{\\partial}{\\partial w_{j,0}}\n\\left(\n\\frac{1}{2m}\n\\sum_{i=1}^{m}\n\\left(\ny_i - w_{j,1}x_i - w_{j,0}\n\\right)^2\n\\right)\n=\n\\frac{1}{m}\n\\sum_{i=1}^{m}\n\\left(\nw_{j,1}x_i + w_{j,0} - y_i\n\\right)\n$$\n:::\n\n::: {.callout-note title=\"Para la estimación de $w_{j,1}$\" collapsible=\"false\"}\n\n$$\nw_{j+1,1} = w_{j,1} - \\alpha \\frac{\\partial J}{\\partial w_{j,1}}\n$$\n\n$$\n\\frac{\\partial J}{\\partial w_{j,1}} =\n\\frac{\\partial}{\\partial w_{j,1}}\n\\left(\n\\frac{1}{2m} \\sum_{i=1}^{m}\n\\left( y_i - w_{j,1} x_i - w_{j,0} \\right)^2\n\\right)\n$$\n\n$$\n\\frac{\\partial J}{\\partial w_{j,1}}\n=\n\\frac{1}{m}\n\\sum_{i=1}^{m}\n\\left(\nw_{j,1}x_i + w_{j,0} - y_i\n\\right)x_i\n$$\n\n:::\n\n:::\n\n\n---\n\n## Variantes: batch, mini-batch, SGD\n\n- **Batch GD:** usa todo el conjunto en cada actualización (costo alto por iteración).\n- **Mini-batch GD:** usa lotes pequeños (compromiso eficiencia/ruido).\n- **SGD (estocástico):** actualiza con una sola muestra por paso (barato, ruidoso, puede escapar de óptimos pobres).\n\n**Práctica recomendada:** mini-batch (p. ej., 32–256).\n\n---\n\n## Preprocesamiento y escalado\n\n- **Estandarizar** o **normalizar** las $x_j$ acelera y estabiliza GD.\n- Centrar: $x_j \\leftarrow (x_j - \\mu_j)/\\sigma_j$.\n- Manejo de outliers y transformaciones (log, Box–Cox) cuando aplique.\n\n::: notes\nConectar con mediciones biomédicas heterogéneas y escalas físicas.\n:::\n\n---\n\n## 1.5 EDA\n\n### Empezar a usar jupyter.\n\n---\n\n# 2. Caso 1 · Regresión lineal múltiple con GD\n\n## Planteamiento\n\n**Objetivo biomédico (ejemplo):** predecir **gasto energético** (kcal) a partir de **edad**, **IMC** y **FC**.\n\nModelo lineal:\n$$\n\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} = w_0 + w_1 x_1 + \\cdots + w_p x_p\n$$\n\nCosto (MSE):\n$$\nJ(\\mathbf{w}) = \\frac{1}{2m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)^2\n$$\n\nGradiente:\n$$\n\\frac{\\partial J}{\\partial w_j} = -\\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat{y}_i)\\,x_{ij}\n$$\n\nActualización:\n$$\nw_j \\leftarrow w_j - \\alpha\\,\\frac{\\partial J}{\\partial w_j}\n$$\n\n---\n\n## Pseudocódigo (mini-batch)\n\n```pseudo\nin: X (m×p), y (m), α, batch_size, epochs\npreprocess: X ← standardize(X)\n\ninitialize w ← zeros(p+1)  # incluye sesgo w0 si se usa X̃ con columna 1\n\nfor epoch in 1..epochs:\n    for B in iterate_minibatches(X, y, batch_size, shuffle=True):\n        Xb, yb ← B\n        yhat ← Xb · w\n        grad ← (1/|B|) · (Xbᵀ · (yhat - yb))\n        w ← w - α · grad\n\nreturn w\n```\n\n::: notes\nDiscutir convergencia, criterio de parada (máx. iteraciones o ΔJ pequeño).\n:::\n\n---\n\n## Diagnóstico y validación\n\n- Curva $J$ vs. iteraciones (entrenamiento y validación).\n- Errores residuales: homocedasticidad, estructura vs. predicción.\n- Interpretación clínica de coeficientes $w_j$ y unidades.\n- Comparar con **ecuaciones normales** (solución cerrada) y discutir condicionamiento numérico.\n\n---\n\n# 3. Caso 2 · Regresión logística con GD (clasificación 0/1)\n\n## Planteamiento\n\n**Objetivo biomédico (ejemplo):** clasificar **riesgo de enfermedad** (0/1) con panel de biomarcadores.\n\nModelo:\n$$\n\\hat{p} = \\sigma(\\mathbf{w}^\\top\\mathbf{x}),\\quad \\sigma(z)=\\frac{1}{1+e^{-z}}\n$$\n\nCosto (entropía cruzada):\n$$\nJ(\\mathbf{w}) = -\\frac{1}{m}\\sum_{i=1}^{m}\\Big[y_i\\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i)\\Big]\n$$\n\nGradiente:\n$$\n\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{p}_i - y_i)\\,x_{ij}\n$$\n\nActualización:\n$$\nw_j \\leftarrow w_j - \\alpha\\,\\frac{\\partial J}{\\partial w_j}\n$$\n\n---\n\n## Pseudocódigo (mini-batch)\n\n```pseudo\nin: X (m×p), y∈{0,1}^m, α, batch_size, epochs\npreprocess: X ← standardize(X)\n\ninitialize w ← zeros(p+1)\n\nfor epoch in 1..epochs:\n    for B in iterate_minibatches(X, y, batch_size, shuffle=True):\n        Xb, yb ← B\n        z ← Xb · w\n        p ← sigmoid(z)\n        grad ← (1/|B|) · (Xbᵀ · (p - yb))\n        w ← w - α · grad\n\nreturn w\n```\n\n**Inferencia:** clasificar con umbral $\\hat{p} \\ge \\tau$ (clínico/operativo).\n\n---\n\n## Métricas y curvas\n\n- **AUC-ROC**, **AUPRC**, **sensibilidad**, **especificidad**, **F1**.\n- Elección del **umbral $\\tau$** por criterio clínico (p. ej., maximizar sensibilidad bajo límite de FPs).\n- Calibración: curvas de confiabilidad.\n\n::: notes\nRelaciones costo-beneficio y prevalencia en biomédica.\n:::\n\n---\n\n## Visualización de la frontera de decisión\n\n- Con dos características ($x_1, x_2$), la frontera es una **línea** (hiperplano en general).\n- Durante GD, la frontera rota/traslada hasta estabilizarse.\n- Añadir **términos polinomiales** o **bases** para fronteras no lineales; GD sigue aplicando.\n\n---\n\n# 4. Buenas prácticas y discusión aplicada\n\n## Hiperparámetros y trucos prácticos\n\n- **Tasa de aprendizaje ($\\alpha$)**: búsqueda en rejilla o **programación de tasa** (decay).\n- **Inicialización**: pequeña aleatoria (cero puede estancar con ciertas variantes).\n- **Barajado** por época y **mini-batches** estratificados si la clase es rara.\n- **Regularización** (L2/L1) para estabilidad e interpretabilidad:\n  $$\n  J_{\\lambda} = J + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|_2^2 \\quad \\text{(Ridge)}\n  $$\n- **Detección de fuga de datos** y validación por **sujeto** en estudios clínicos.\n\n---\n\n## Checklist de la sesión (rápido)\n\n- [ ] Estandarizaste variables de entrada.\n- [ ] Definiste costo adecuado (MSE vs. CE).\n- [ ] Elegiste mini-batch y $\\alpha$ razonables.\n- [ ] Verificaste convergencia con curva de $J$.\n- [ ] Evaluaste con métricas adecuadas al objetivo clínico.\n- [ ] Documentaste supuestos y limitaciones.\n\n---\n",
    "supporting": [
      "v2Lect002_AlgoritmoDescensoGradiente_files/figure-beamer"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{longtable}\n\\usepackage{array}\n\\usepackage{multirow}\n\\usepackage{wrapfig}\n\\usepackage{float}\n\\usepackage{colortbl}\n\\usepackage{pdflscape}\n\\usepackage{tabu}\n\\usepackage{threeparttable}\n\\usepackage{threeparttablex}\n\\usepackage[normalem]{ulem}\n\\usepackage{makecell}\n\\usepackage{xcolor}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}