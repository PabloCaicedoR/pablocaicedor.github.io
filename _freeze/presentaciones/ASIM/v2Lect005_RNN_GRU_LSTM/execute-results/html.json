{
  "hash": "ac6976615253492794bc3b6383ddec00",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Arquitecturas de Memoria Profunda\"\nsubtitle: \"RNN, LSTM y GRU: Teoría e Implementación\"\nauthor: \"PhD. Pablo Eduardo Caicedo Rodríguez\"\ndate: last-modified\nlang: es\nformat:\n  revealjs:\n    code-tools: true\n    code-overflow: wrap\n    code-line-numbers: true\n    code-copy: true\n    fig-align: center\n    self-contained: true\n    theme:\n      - simple\n      - ../../recursos/estilos/metropolis.scss\n    slide-number: true\n    preview-links: auto\n    logo: ../../recursos/imagenes/generales/Escuela_Rosario_logo.png\n    css: ../../recursos/estilos/styles_pres.scss\n    footer: <https://pablocaicedor.github.io/>\n    transition: fade\n    progress: true\n    scrollable: true\n    hash: true\n\nexecute:\n  echo: false\n  warning: false\n  freeze: auto\n\n---\n\n\n## 1. La Dimensión Temporal en IA\n\n::: {.incremental}\n- **El Paradigma Estático:** Las redes *feedforward* tradicionales asumen independencia entre muestras (ej. clasificación de imágenes estáticas).\n- **La Realidad Dinámica:** El mundo físico y cognitivo es secuencial (lenguaje, audio, señales biológicas, finanzas).\n- **La Necesidad de Memoria:** Se requiere una arquitectura capaz de mantener un \"estado\" o contexto histórico para interpretar el presente.\n:::\n\n---\n\n## 2. Redes Neuronales Recurrentes (RNN)\n\n### Concepto Fundamental\n\nLa RNN rompe la estructura acíclica. Al \"desenrollar\" la red en el tiempo, procesa la secuencia paso a paso compartiendo los mismos pesos ($W$).\n\n::: {.fragment}\n$$h_t = \\sigma_h (W_{ih} x_t + W_{hh} h_{t-1} + b)$$\n:::\n\n::: {.columns}\n::: {.column width=\"45%\"}\n*   **$h_t$**: Estado oculto (memoria) actual.\n*   **$h_{t-1}$**: Estado previo.\n*   **$x_t$**: Entrada actual.\n:::\n::: {.column width=\"45%\"}\n*   **$W_{hh}$**: Matriz recurrente (Memoria).\n*   **$\\sigma_h$**: Activación ($\\tanh$).\n:::\n:::\n\n---\n\n## El Problema del Gradiente\n\nEntrenamiento mediante *Backpropagation Through Time* (BPTT).\n\nLa derivada del error respecto a los pesos recurrentes implica un producto continuo de matrices Jacobianas:\n\n$$ \\frac{\\partial h_t}{\\partial h_k} = \\prod_{j=k+1}^t \\text{diag}(\\sigma'(z_j)) W_{hh} $$\n\n::: {.callout-warning}\n### Patologías Espectrales\n*   **Desvanecimiento ($\\rho < 1$):** El gradiente tiende a cero. La red \"olvida\" relaciones lejanas (ej. sujeto vs. verbo en párrafos largos).\n*   **Explosión ($\\rho > 1$):** El gradiente diverge. Requiere *Gradient Clipping*.\n:::\n\n---\n\n## 3. Long Short-Term Memory (LSTM)\n\n### Ingeniería de la Persistencia\n\nDiseñada para mitigar el desvanecimiento del gradiente introduciendo una **Celda de Memoria ($C_t$)** separada del estado oculto.\n\n::: {.block}\n**Innovación Clave:** El \"Carrusel de Error Constante\". La información fluye por la celda con interacciones lineales (sumas), protegiendo la señal del gradiente.\n:::\n\n---\n\n## La neurona LSTM\n\n![Tomada de codificando bits](../../recursos/imagenes/Presentaciones/ASIM/lstm.jpg)\n\n\n\n## Anatomía de la LSTM (1/2)\n\nEl flujo es regulado por **Compuertas Sigmoidales** ($\\sigma \\in [0, 1]$):\n\n::: {.columns}\n::: {.column width=\"45%\"}\n#### 1. Compuerta de Olvido ($f_t$)\n¿Qué borramos del pasado?\n$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n\n#### 2. Compuerta de Entrada ($i_t$)\n¿Qué información nueva guardamos?\n$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n:::\n\n::: {.column width=\"45%\"}\n#### Memoria Candidata ($\\tilde{C}_t$)\nPropuesta de nuevo contenido.\n$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n:::\n:::\n\n---\n\n## Anatomía de la LSTM (2/2)\n\n### Actualización y Salida\n\n#### 3. Actualización de Celda ($C_t$)\nCombinación lineal crítica. Aquí ocurre la magia de la persistencia.\n$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n\n#### 4. Generación de Salida ($h_t$)\nFiltrado de la memoria para el uso inmediato.\n$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n$$h_t = o_t \\odot \\tanh(C_t)$$\n\n---\n\n## 4. Gated Recurrent Units (GRU)\n\n### Eficiencia y Simplificación\n\nPropuesta para reducir la complejidad computacional de la LSTM sin sacrificar rendimiento.\n\n*   **Fusión de Estados:** No hay celda $C_t$ separada. Solo existe $h_t$.\n*   **Reducción de Compuertas:** Solo dos compuertas (*Update* y *Reset*).\n\n---\n\n## Dinámica de la GRU\n\n::: {.columns}\n::: {.column width=\"45%\"}\n**Compuerta de Actualización ($z_t$):**\nFusiona las funciones de entrada y olvido.\n$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$$\n\n**Compuerta de Reinicio ($r_t$):**\nDecide cuánto del pasado ignorar para el cálculo actual.\n$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$$\n:::\n\n::: {.column width=\"45%\"}\n**Interpolación Final:**\n$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n\nPermite saltar pasos temporales o sobrescribir memoria completamente.\n:::\n:::\n\n---\n\n## Comparativa Técnica\n\n| Característica | LSTM | GRU |\n| :--- | :--- | :--- |\n| **Complejidad** | Alta (3 compuertas) | Media (2 compuertas) |\n| **Parámetros** | $4 \\times ((n+m)n + n)$ | $\\approx 25\\%$ menos |\n| **Memoria** | Estado Oculto + Celda | Solo Estado Oculto |\n| **Dependencias** | Extremadamente largas | Largas / Medias |\n| **Datos Requeridos** | Alto volumen | Funciona bien con *Small Data* |\n\n---\n\n## 5. Implementación en PyTorch\n\nEstructura modular agnóstica a la arquitectura (\"Many-to-One\").\n\n::: {#fba18f46 .cell execution_count=1}\n``` {.python .cell-code code-line-numbers=\"6-22|24-37\"}\nimport torch.nn as nn\n\nclass ClasificadorRecurrente(nn.Module):\n    def __init__(self, tipo_rnn, input_size, hidden_size, num_classes):\n        super().__init__()\n\n        # Selección Dinámica de Arquitectura\n        if tipo_rnn == 'LSTM':\n            self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n        elif tipo_rnn == 'GRU':\n            self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n        else:\n            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        # x shape: (Batch, Seq_Len, Features)\n\n        # Propagación\n        if isinstance(self.rnn, nn.LSTM):\n            out, (hn, cn) = self.rnn(x) # LSTM devuelve tupla\n        else:\n            out, hn = self.rnn(x)       # GRU/RNN devuelven tensor\n\n        # Clasificación basada en el último paso de tiempo\n        last_step = out[:, -1, :]\n        return self.fc(last_step)\n```\n:::\n\n\n---\n\n## 6. RNNs en la Era de Transformers\n\n¿Por qué seguir usando RNN/GRU hoy en día?\n\n::: {.incremental}\n1.  **Complejidad Computacional:**\n    *   Transformers: $O(N^2)$ en tiempo/memoria (o Caché KV lineal).\n    *   RNN/GRU: $O(1)$ en memoria durante inferencia.\n2.  **Edge AI / TinyML:**\n    *   Ideales para microcontroladores (Wearables, Sensores IoT).\n3.  **Streaming:**\n    *   Procesamiento de audio/señales en tiempo real con latencia cero (Wake Word Detection).\n:::\n\n---\n\n## Conclusión\n\n::: {.callout-note}\n### Resumen\n*   **RNN:** Fundamento teórico del aprendizaje secuencial. Inestable en secuencias largas.\n*   **LSTM:** Estándar de oro para dependencias complejas gracias a su celda de memoria protegida.\n*   **GRU:** La opción eficiente. Menor coste computacional, ideal para inferencia rápida y *Edge Computing*.\n:::\n\nLa elección arquitectónica depende del balance entre **capacidad de memoria** y **restricciones de latencia/hardware**.\n\n",
    "supporting": [
      "v2Lect005_RNN_GRU_LSTM_files"
    ],
    "filters": [],
    "includes": {}
  }
}