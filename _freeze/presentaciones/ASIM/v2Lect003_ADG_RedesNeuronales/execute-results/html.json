{
  "hash": "372bc52ecea15c464ecdd1686d860e74",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Descenso de Gradiente en Ciencias Biomédicas\"\nsubtitle: \"Regresión lineal múltiple, regresión logística y Redes Neuronales\"\nauthor: \"PhD. Pablo Eduardo Caicedo Rodríguez\"\ndate: last-modified\nlang: es\nformat:\n  revealjs:\n    code-tools: true\n    code-overflow: wrap\n    code-line-numbers: true\n    code-copy: true\n    fig-align: center\n    self-contained: true\n    theme:\n      - simple\n      - ../../recursos/estilos/metropolis.scss\n    slide-number: true\n    preview-links: auto\n    logo: ../../recursos/imagenes/generales/Escuela_Rosario_logo.png\n    css: ../../recursos/estilos/styles_pres.scss\n    footer: <https://pablocaicedor.github.io/>\n    transition: fade\n    progress: true\n    scrollable: true\n    hash: true\n  beamer:\n    slide-level: 2\n    incremental: false\n    aspectratio: 32\n    navigation: horizontal\n    theme: CambridgeUS\n    header-includes:\n      \\makeatletter\n      \\expandafter\\let\\csname figure*\\endcsname\\figure\n      \\expandafter\\let\\csname endfigure*\\endcsname\\endfigure\n      \\expandafter\\let\\csname table*\\endcsname\\table\n      \\expandafter\\let\\csname endtable*\\endcsname\\endtable\nexecute:\n  echo: false\n  warning: false\n  freeze: auto\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n---\n\n## Objetivos de aprendizaje\n\n- **Comprender** los fundamentos de la **Regresión Lineal**, **Regresión Logística** y **Perceptrón Multicapa (MLP)**.\n- **Aplicar** estos modelos al contexto de **salud fetal** con datos de **cardiotocografía (CTG)**.\n- **Evaluar** el desempeño con métricas adecuadas (MSE, AUC/Log-Loss, matriz de confusión, F1).\n\n::: {.columns}\n::: {.column width=\"45%\"}\n\n1. Regresión Lineal\n2. Regresión Logística\n3. Perceptrón Multicapa\n4. Cierre y discusión\n\n:::\n::: {.column width=\"45%\"}\n**Dataset**: `fetal_health.csv` (UCI CTG).\n**Contexto clínico**: interpretación de CTG (*normal*, *sospechoso*, *patológico*).\n:::\n:::\n\n---\n\n## Contexto clínico (CTG)\n\n::: {.callout-important title=\"Defición\"}\n Prueba médica que monitoriza simultáneamente la frecuencia cardíaca del feto y la actividad contráctil del útero. Se realiza generalmente durante el tercer trimestre del embarazo y el parto, colocando dos transductores externos (uno para la frecuencia cardíaca fetal y otro para las contracciones) sobre el abdomen de la madre\n:::\n\n![Generada con Gemini](../../recursos/imagenes/Presentaciones/ASIM/cardiotocografia.png)\n\n## Contexto clínico (CTG)\n\n|Característica (Variable en CSV)                      |Cálculo o Descripción                                                                                                                                                                                                                                                                                       |\n|------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|Parámetros Basales                                    |                                                                                                                                                                                                                                                                                                            |\n|baseline value                                        |Es la frecuencia cardíaca fetal (FCF) media aproximada en un segmento de 10 minutos, excluyendo aceleraciones, deceleraciones y períodos de variabilidad marcada (>25 lpm). Se redondea a incrementos de 5 latidos por minuto (lpm).[4, 5, 6, 7, 8] El rango normal se considera entre 110 y 160 lpm.[9, 10]|\n|fetal_movement                                        |Número de movimientos fetales detectados por segundo.[1, 11, 12]                                                                                                                                                                                                                                            |\n|uterine_contractions                                  |Número de contracciones uterinas por segundo. Se considera normal tener 5 o menos contracciones en 10 minutos.[1, 4, 11, 12]                                                                                                                                                                                |\n|Eventos Transitorios (Aceleraciones y Deceleraciones) |                                                                                                                                                                                                                                                                                                            |\n|accelerations                                         |Número de aceleraciones por segundo. Una aceleración es un aumento abrupto de la FCF por encima de la línea de base de al menos 15 lpm, que dura 15 segundos o más, pero menos de 2 minutos.[5, 9, 10]                                                                                                      |\n|light_decelerations                                   |Número de deceleraciones leves por segundo. Una deceleración es una caída de la FCF de más de 15 lpm que dura más de 15 segundos.[5] La categoría \"leve\" se refiere a su duración, típicamente menor a 120 segundos.[3]                                                                                     |\n|severe_decelerations                                  |Número de deceleraciones severas por segundo. Se refiere a deceleraciones de larga duración, a menudo definidas como aquellas que superan los 300 segundos.[3]                                                                                                                                              |\n|prolongued_decelerations                              |Número de deceleraciones prolongadas por segundo. Son caídas de la FCF que duran más de 2 o 3 minutos pero menos de 10 minutos.[3, 6, 13]                                                                                                                                                                   |\n|Variabilidad de la FCF                                |                                                                                                                                                                                                                                                                                                            |\n|abnormal_short_term_variability                       |Porcentaje de tiempo en que la variabilidad a corto plazo (latido a latido) es anormal. La variabilidad se considera anormal si es mínima (≤5 lpm) o marcada (>25 lpm).[6, 8]                                                                                                                               |\n|mean_value_of_short_term_variability                  |Valor medio de la variabilidad a corto plazo (STV), que describe las fluctuaciones de la FCF latido a latido.[3, 6]                                                                                                                                                                                         |\n|percentage_of_time_with_abnormal_long_term_variability|Porcentaje de tiempo en que la variabilidad a largo plazo es anormal. Se calcula sobre las fluctuaciones de la FCF en un período de un minuto.[5]                                                                                                                                                           |\n|mean_value_of_long_term_variability                   |Valor medio de la variabilidad a largo plazo (LTV), que mide la amplitud (diferencia entre el pico y el valle) de las fluctuaciones de la FCF en un minuto.[3, 5]                                                                                                                                           |\n|Características del Histograma de FCF                 |Estas son propiedades estadísticas calculadas a partir de la distribución de todos los valores de FCF registrados durante el período de monitorización.[1, 11, 12]                                                                                                                                          |\n|histogram_width                                       |El ancho del histograma, calculado como la diferencia entre el valor máximo (histogram_max) y el mínimo (histogram_min) de la FCF.                                                                                                                                                                          |\n|histogram_min                                         |El valor mínimo de la FCF registrado en el histograma.                                                                                                                                                                                                                                                      |\n|histogram_max                                         |El valor máximo de la FCF registrado en el histograma.                                                                                                                                                                                                                                                      |\n|histogram_number_of_peaks                             |El número de picos en la distribución del histograma.                                                                                                                                                                                                                                                       |\n|histogram_number_of_zeroes                            |El número de \"ceros\" o bins con frecuencia cero en el histograma.                                                                                                                                                                                                                                           |\n|histogram_mode                                        |El valor de FCF que aparece con mayor frecuencia (la moda estadística).                                                                                                                                                                                                                                     |\n|histogram_mean                                        |El valor medio de la FCF en el histograma (la media estadística).                                                                                                                                                                                                                                           |\n|histogram_median                                      |El valor central de la FCF en el histograma (la mediana estadística).                                                                                                                                                                                                                                       |\n|histogram_variance                                    |La varianza de los valores de FCF, que mide su dispersión alrededor de la media.                                                                                                                                                                                                                            |\n|histogram_tendency                                    |Indica la simetría o sesgo del histograma. Puede interpretarse como: 1 para tendencia a la derecha (positiva), -1 para tendencia a la izquierda (negativa) y 0 para una distribución simétrica.                                                                                                             |\n\n\n## Contexto clínico (CTG)\n\n- CTG registra **FCF** y **contracciones uterinas**.\n- Clasificación clínica (FIGO): **normal / sospechoso / patológico**.\n- Variabilidad, aceleraciones y desaceleraciones son claves.\n\n\n\n---\n\n## Regresión Lineal\n\n### Idea clave\nAproxima una relación **lineal** $\\hat{y} = \\beta_0 + \\sum_j \\beta_j x_j$minimizando **MSE**.\n\n### Ejemplo didáctico (CTG)\nUsamos una **variable continua** de CTG como respuesta (p. ej., *histogram_width*) para ilustrar ajuste y residuales.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](v2Lect003_ADG_RedesNeuronales_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n**Discusión:** supuestos (linealidad, homocedasticidad, independencia), diagnóstico con residuales.\n\n---\n\n## Regresión Logística\n\n### Idea clave\nModela $P(Y=1 \\mid \\mathbf{x}) = \\sigma(\\beta_0 + \\mathbf{x}^\\top \\beta)$ con **sigmoide** $\\sigma(z)=1/(1+e^{-z})$.\n\n## 1. Definición de Regresión Logística\n\nLa **Regresión Logística** es un algoritmo de aprendizaje automático supervisado utilizado fundamentalmente para problemas de **clasificación binaria**.\n\nA pesar de su nombre, su objetivo no es predecir un valor continuo, sino modelar la **probabilidad** ($P$) de que una observación pertenezca a una clase específica (usualmente denotada como $Y=1$).\n\nEl modelo toma variables de entrada (features) $x_1, \\dots, x_n$ y estima $P(Y=1 | \\mathbf{x})$.\n\n---\n\n## 2. El Mecanismo Central del Modelo\n\nEl modelo logístico opera en dos pasos cruciales:\n\n### 2.1. El Componente Lineal (Logit)\n\nPrimero, el modelo calcula una suma ponderada de las entradas, exactamente igual que en una regresión lineal. A este resultado ($z$) se le conoce como **logit** o, más formalmente, **log-odds**.\n\n$$\nz = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n$$\n\n* $\\beta_0$ es el intercepto (sesgo).\n* $\\beta_{1 \\dots n}$ son los coeficientes (pesos) que el modelo aprende.\n* El rango de salida de $z$ es el de todos los números reales: $(-\\infty, +\\infty)$.\n\n### 2.2. La Función Sigmoide (Logística)\n\nDado que una probabilidad debe estar en el rango $[0, 1]$, $z$ no puede ser el resultado final. La regresión logística aplica la **función sigmoide** ($\\sigma$) a $z$ para \"aplastar\" (squash) la salida lineal al rango de probabilidad.\n\n$$\nP = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n$$\n\n* Si $z \\to +\\infty$, $e^{-z} \\to 0$, y $P \\to 1$.\n* Si $z \\to -\\infty$, $e^{-z} \\to +\\infty$, y $P \\to 0$.\n* Si $z = 0$, $e^{-0} = 1$, y $P = 0.5$.\n\n---\n\n## 3. La Relación Clave: Probabilidad y Log-Odds\n\nEl concepto central que conecta el modelo lineal con la probabilidad es el **log-odds**. Esta transformación es necesaria para mapear un espacio acotado $[0, 1]$ a un espacio no acotado $[-\\infty, +\\infty]$.\n\n### 3.1. De Probabilidad a Log-Odds\n\nLa transformación se realiza en dos pasos:\n\n1.  **Probabilidad ($P$)**: La probabilidad del evento.\n    * Rango: $[0, 1]$\n\n2.  **Odds (Momios)**: La razón entre la probabilidad de que ocurra ($P$) y la de que no ocurra ($1-P$).\n    $$\n    Odds = \\frac{P}{1-P}\n    $$\n    * Rango: $[0, +\\infty]$\n\n3.  **Log-Odds (Logit)**: El logaritmo natural de los *odds*.\n    $$\n    Logit(P) = \\ln(Odds) = \\ln\\left(\\frac{P}{1-P}\\right)\n    $$\n    * Rango: $[-\\infty, +\\infty]$\n\nEl modelo de regresión logística es, por tanto, un modelo lineal que predice el *log-odds*:\n\n$$\nz = \\ln\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\n$$\n\n### 3.2. De Log-Odds a Probabilidad (La Inversa)\n\nPara obtener la probabilidad $P$ a partir del log-odds $z$, simplemente revertimos la transformación `Logit`. Este proceso de despejar $P$ de la ecuación del logit **da origen a la función sigmoide**:\n\n1.  Ecuación base:\n    $$\n    z = \\ln\\left(\\frac{P}{1-P}\\right)\n    $$\n\n2.  Aplicar exponencial (inversa del logaritmo):\n    $$\n    e^z = \\frac{P}{1-P}\n    $$\n\n3.  Despejar $P$:\n    $$\n    e^z (1-P) = P\n    $$\n    $$\n    e^z - e^z P = P\n    $$\n    $$\n    e^z = P + e^z P\n    $$\n    $$\n    e^z = P (1 + e^z)\n    $$\n\n4.  Probabilidad $P$ en función de $z$:\n    $$\n    P = \\frac{e^z}{1 + e^z}\n    $$\n\n5.  *Forma sigmoide alternativa (dividiendo numerador y denominador por $e^z$)*:\n    $$\n    P = \\frac{e^z/e^z}{(1 + e^z)/e^z} = \\frac{1}{e^{-z} + 1} = \\frac{1}{1 + e^{-z}}\n    $$\n\n---\n\n## 4. Interpretación de Coeficientes\n\nDebido a esta relación, los coeficientes ($\\beta_i$) del modelo tienen una interpretación específica:\n\n* **Coeficiente $\\beta_i$**: Un incremento de una unidad en la variable $x_i$ (manteniendo las demás constantes) genera un cambio de $\\beta_i$ en el **log-odds** de la predicción.\n* **Odds Ratio (OR)**: Para una interpretación más intuitiva, se utiliza $e^{\\beta_i}$. Un incremento de una unidad en $x_i$ **multiplica** los *odds* por un factor de $e^{\\beta_i}$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](v2Lect003_ADG_RedesNeuronales_files/figure-revealjs/unnamed-chunk-3-3.png){width=960}\n:::\n:::\n\n\n### Clasificación binaria (Normal vs No‑Normal)\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n              precision    recall  f1-score   support\n\n           0      0.778     0.745     0.761        94\n           1      0.929     0.940     0.934       332\n\n    accuracy                          0.897       426\n   macro avg      0.853     0.842     0.848       426\nweighted avg      0.895     0.897     0.896       426\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](v2Lect003_ADG_RedesNeuronales_files/figure-revealjs/unnamed-chunk-4-5.png){width=960}\n:::\n:::\n\n\n---\n\n# Redes Neuronales\n\n## 1. El Punto de Partida: Regresión Lineal\n\nEl problema de regresión estándar busca encontrar una función $f$ que mapee entradas $\\mathbf{x}$ a una salida $y$.\n\nEn la **Regresión Lineal**, asumimos que la relación es *lineal*:\n\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n$$\nO en forma vectorial:\n$$\ny = \\mathbf{\\beta}^T \\mathbf{x} + \\beta_0\n$$\n\n* **Objetivo**: Encontrar los parámetros óptimos ($\\mathbf{\\beta}, \\beta_0$) que minimizan una función de pérdida (ej. Error Cuadrático Medio, MSE).\n* **Limitación**: El modelo está restringido a representar únicamente relaciones lineales.\n\n---\n\n## 1. El Punto de Partida: Regresión Lineal\n\n![Esquema Regresión Lineal](../../recursos/imagenes/Presentaciones/ASIM/esquemaRegresionLineal.png)\n\n---\n\n\n## 2. Primera Generalización: El GLM\n\n¿Qué pasa si la salida no es lineal o no sigue una distribución normal? (Ej. clasificación binaria).\n\nUsamos un **Modelo Lineal Generalizado (GLM)**, como la **Regresión Logística**:\n\n1.  **Predictor Lineal ($z$)**: Mantenemos el núcleo lineal.\n    $$\n    z = \\mathbf{\\beta}^T \\mathbf{x} + \\beta_0\n    $$\n    El resultado $z$ (el *logit*) puede ir de $(-\\infty, +\\infty)$.\n\n2.  **Función de Enlace (Inversa)**: Aplicamos una transformación no-lineal $g^{-1}$ para mapear $z$ al rango deseado (ej. $[0, 1]$ para probabilidad).\n    $$\n    \\hat{y} = g^{-1}(z) = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n    $$\n\n* **Avance**: Hemos *generalizado* la regresión. El modelo sigue siendo lineal en sus parámetros $\\mathbf{w}$, pero puede modelar fenómenos no lineales mediante una **función de activación** (la sigmoide).\n\n---\n\n## 2. Primera Generalización: El GLM\n\n![Esquema Regresión Logistica](../../recursos/imagenes/Presentaciones/ASIM/esquemaRegresionLogistica.png)\n\n---\n\n## 3. La Neurona: Una Unidad de Regresión\n\nUna **neurona artificial** (o Perceptrón) es conceptualmente idéntica a un modelo de regresión logística (un GLM).\n\n::: {.incremental}\n1.  **Agregación Lineal**: Calcula el predictor lineal $z$ (la entrada neta).\n    $$\n    z = \\sum_{i=1}^n w_i x_i + b\n    $$\n\n2.  **Función de Activación**: Aplica una transformación no-lineal $a$ (la salida).\n    $$\n    a = g(z)\n    $$\n:::\n\n\n\n* $g(z)$ puede ser $\\text{sigmoid}(z)$, $\\tanh(z)$, $\\text{ReLU}(z)$, etc.\n* **Concepto Clave**: Una sola neurona es una unidad de regresión (lineal o generalizada) que aprende un *único* límite de decisión (o una respuesta lineal).\n\n---\n\n## 3. La Neurona: Una Unidad de Regresión\n\n![Esquema Neurona Artificial](../../recursos/imagenes/Presentaciones/ASIM/esquemaNeurona.png)\n\n---\n\n## 4. La Red: Composición de Funciones\n\n¿Cómo modelar relaciones *altamente* complejas que una sola neurona no puede capturar?\n\n**Respuesta**: Apilando las neuronas en capas.\n\nLa salida de una capa de \"unidades de regresión\" se convierte en la *entrada* de la siguiente capa.\n\n$$\n\\text{Capa 1 (Oculta): } \\mathbf{a}^{(1)} = g_1(\\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)})\n$$\n$$\n\\text{Capa 2 (Salida): } \\hat{y} = g_2(\\mathbf{W}^{(2)}\\mathbf{a}^{(1)} + \\mathbf{b}^{(2)})\n$$\n\nEsto es una **composición de funciones** anidada:\n\n$$\n\\hat{y} = g_2\\left( \\mathbf{W}^{(2)} \\left( g_1(\\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}) \\right) + \\mathbf{b}^{(2)} \\right)\n$$\n\n---\n\n## 5. El Teorema de Aproximación Universal\n\nEsta arquitectura de \"regresiones apiladas\" tiene una propiedad fundamental:\n\n> **Teorema de Aproximación Universal**: Una red neuronal *feedforward* con una sola capa oculta (con una función de activación no lineal, como ReLU o Sigmoide) puede aproximar cualquier función continua $f(\\mathbf{x})$ a un grado arbitrario de precisión, dado un número suficiente de neuronas.\n\n* Una regresión lineal solo puede encontrar la *mejor línea*.\n* Una red neuronal puede encontrar (aprender) la *función* $f(\\mathbf{x})$ en sí misma, sin importar su forma.\n\n**La red neuronal es un regresor generalizado en el sentido más amplio.**\n\n---\n\n## 6. Generalización del Aprendizaje (Optimización)\n\nEl proceso de \"entrenamiento\" también es una generalización de la regresión.\n\n::: {.columns}\n::: {.column width=\"45%\"}\n\n::: {.small-font}\n**Regresión (Lineal/Logística)**\n\n* **Pérdida**: Se define una función de coste (Loss Function) $L(y, \\hat{y})$.\n    * *Ej. MSE*: $L = (y - \\hat{y})^2$\n    * *Ej. Cross-Entropy*: $L = -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$\n* **Optimización**: Se encuentran los $\\mathbf{w}$ y $b$ que minimizan $L$ (a menudo usando Descenso de Gradiente).\n:::\n\n:::\n\n::: {.column width=\"45%\"}\n\n:::{.small-font}\n\n**Red Neuronal**\n\n* **Pérdida**: Se define la *misma* función de coste $L(y, \\hat{y})$ en la salida final.\n* **Optimización**: Se encuentran *todos* los $\\mathbf{W}^{(l)}$ y $\\mathbf{b}^{(l)}$ de *todas* las capas que minimizan $L$.\n* **Mecanismo**: **Descenso de Gradiente** + **Backpropagation** (Retropropagación).\n    * Backpropagation es simplemente la aplicación sistemática de la **regla de la cadena** del cálculo para encontrar el gradiente de $L$ con respecto a cada peso en la vasta función compuesta.\n:::\n\n:::\n\n:::\n\n---\n\n## 7. Conclusión: La Red como $f_{\\theta}(\\mathbf{x})$\n\n1.  **Regresión Lineal**:\n    * Modelo: $f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b$\n    * Asume una forma funcional *fija* (lineal).\n\n2.  **GLM (Logística)**:\n    * Modelo: $f(\\mathbf\n    {x}) = g(\\mathbf{w}^T \\mathbf{x} + b)$\n    * Asume una forma fija (lineal) transformada por una activación fija (ej. sigmoide).\n\n3.  **Red Neuronal (MLP)**:\n    * Modelo: $f(\\mathbf{x}) = f_L(\\dots f_2(f_1(\\mathbf{x}))\\dots)$\n    * **No asume una forma funcional fija**.\n    * Es un **aproximador de funciones universal** cuyos parámetros $\\theta$ (todos los $W$ y $b$) se *aprenden* para que $f_{\\theta}(\\mathbf{x})$ imite la verdadera función subyacente $f(\\mathbf{x})$ en los datos.\n\nLa red neuronal es la generalización última de la regresión: un sistema que **aprende la forma de la función** desde los datos.\n\n## Perceptrón Multicapa — MLP\n\n**Arquitectura propuesta:** 21→32→16→3 con *ReLU* y **softmax** para 3 clases $(1,2,3)$.\n\n```{dot}\ndigraph MLP_FetalHealth {\n    rankdir=LR;\n    node [shape=circle];\n\n    subgraph cluster_input {\n        label=\"Input layer (21 CTG features)\";\n        color=gray;\n        i1; i2; i3; i4; i5; i6; i7; i8; i9; i10; i11; i12; i13; i14; i15; i16; i17; i18; i19; i20; i21;\n    }\n\n    subgraph cluster_hidden1 {\n        label=\"Hidden layer 1 (ReLU, 32)\";\n        color=gray;\n        h1_1; h1_2; h1_3; h1_4; h1_5; h1_6; h1_7; h1_8; h1_9; h1_10; h1_11; h1_12; h1_13; h1_14; h1_15; h1_16;\n        h1_17; h1_18; h1_19; h1_20; h1_21; h1_22; h1_23; h1_24; h1_25; h1_26; h1_27; h1_28; h1_29; h1_30; h1_31; h1_32;\n    }\n\n    subgraph cluster_hidden2 {\n        label=\"Hidden layer 2 (ReLU, 16)\";\n        color=gray;\n        h2_1; h2_2; h2_3; h2_4; h2_5; h2_6; h2_7; h2_8; h2_9; h2_10; h2_11; h2_12; h2_13; h2_14; h2_15; h2_16;\n    }\n\n    subgraph cluster_output {\n        label=\"Output layer (Softmax, 3 classes: Normal, Suspect, Pathological)\";\n        color=gray;\n        o1; o2; o3;\n    }\n\n    # Edges (sparse to keep diagram readable)\n    i1 -> {h1_1 h1_2 h1_3 h1_4 h1_5}\n    i2 -> {h1_2 h1_3 h1_4 h1_5 h1_6}\n    i3 -> {h1_3 h1_4 h1_5 h1_6 h1_7}\n    i4 -> {h1_4 h1_5 h1_6 h1_7 h1_8}\n    i5 -> {h1_5 h1_6 h1_7 h1_8 h1_9}\n\n    h1_1 -> {h2_1 h2_2 h2_3 h2_4}\n    h1_5 -> {h2_2 h2_3 h2_4 h2_5}\n    h1_10 -> {h2_4 h2_5 h2_6 h2_7}\n    h1_16 -> {h2_6 h2_7 h2_8 h2_9}\n    h1_24 -> {h2_10 h2_11 h2_12}\n    h1_32 -> {h2_13 h2_14 h2_15 h2_16}\n\n    h2_3 -> o1\n    h2_8 -> o2\n    h2_14 -> o3\n}\n```\n\n### Entrenamiento (multiclase)\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x7f076bd9f770>\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](v2Lect003_ADG_RedesNeuronales_files/figure-revealjs/unnamed-chunk-6-7.png){width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMacro-F1: 0.8003850715462932\n```\n\n\n:::\n:::\n\n\n**Notas didácticas:** desequilibrio de clases (SMOTE), métricas por clase (F1), validación cruzada, *early stopping*.\n\n---\n\n## Cierre y recomendaciones\n\n- La **interpretación clínica** sigue siendo esencial; el ML complementa, no reemplaza, el juicio médico.\n- Validar externamente (dominios LMIC vs HIC), sesgos de muestreo y *data shift*.\n- Reportar métricas por clase, especialmente la **clase sospechosa**.\n\n## Referencias (DOI / ISBN)\n\n- Ayres‑de‑Campos D, *et al.* **FIGO consensus guidelines on intrapartum fetal monitoring: Cardiotocography.** *Int J Gynaecol Obstet* 2015;131(1):13–24. DOI: 10.1016/j.ijgo.2015.06.020\n- Macones GA, *et al.* **The 2008 NICHD workshop report on electronic fetal monitoring.** *J Obstet Gynecol Neonatal Nurs* 2008. (see Obstet Gynecol 2008;112:661–6)\n- Hoodbhoy Z, *et al.* **Use of Machine Learning Algorithms for Prediction of Fetal Risk using Cardiotocographic Data.** *Int J Appl Basic Med Res* 2019;9:226–230. DOI: 10.4103/ijabmr.IJABMR_370_18\n- James G, Witten D, Hastie T, Tibshirani R. **An Introduction to Statistical Learning (2nd ed.).** Springer, 2021. DOI: 10.1007/978-1-0716-1418-1\n- Hosmer DW, Lemeshow S, Sturdivant RX. **Applied Logistic Regression (3rd ed.).** Wiley, 2013. DOI: 10.1002/9781118548387\n- Rumelhart DE, Hinton GE, Williams RJ. **Learning representations by back‑propagating errors.** *Nature* 1986;323:533–536. DOI: 10.1038/323533a0\n- Goodfellow I, Bengio Y, Courville A. **Deep Learning.** MIT Press, 2016. ISBN: 978‑0262035613\n\n---\n\n## Apéndice · Carga de datos y EDA mínimo\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nfetal_health\n1.0    1655\n2.0     295\n3.0     176\nName: count, dtype: int64\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                                     count  ...      max\nbaseline value                                      2126.0  ...  160.000\naccelerations                                       2126.0  ...    0.019\nfetal_movement                                      2126.0  ...    0.481\nuterine_contractions                                2126.0  ...    0.015\nlight_decelerations                                 2126.0  ...    0.015\nsevere_decelerations                                2126.0  ...    0.001\nprolongued_decelerations                            2126.0  ...    0.005\nabnormal_short_term_variability                     2126.0  ...   87.000\nmean_value_of_short_term_variability                2126.0  ...    7.000\npercentage_of_time_with_abnormal_long_term_vari...  2126.0  ...   91.000\nmean_value_of_long_term_variability                 2126.0  ...   50.700\nhistogram_width                                     2126.0  ...  180.000\nhistogram_min                                       2126.0  ...  159.000\nhistogram_max                                       2126.0  ...  238.000\nhistogram_number_of_peaks                           2126.0  ...   18.000\nhistogram_number_of_zeroes                          2126.0  ...   10.000\nhistogram_mode                                      2126.0  ...  187.000\nhistogram_mean                                      2126.0  ...  182.000\nhistogram_median                                    2126.0  ...  186.000\nhistogram_variance                                  2126.0  ...  269.000\nhistogram_tendency                                  2126.0  ...    1.000\nfetal_health                                        2126.0  ...    3.000\n\n[22 rows x 8 columns]\n```\n\n\n:::\n:::\n\n",
    "supporting": [
      "v2Lect003_ADG_RedesNeuronales_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}